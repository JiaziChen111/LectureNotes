\section{Identification of Choice Models}

In order to identify all choice models we are going to see in this chapter, we will need to make several specification assumptions. These assumptions will allow us to go around two main issues that arise because of the choice process. These issues can be summarized in two statements: ``only differences in utility matter'' and ``the scale of utility is arbitrary''.

\subsection{Only Differences in Utility Matter}

Recall the choice process between products as described in the previous chapter: a consumer $i$ will choose a product $j$ over its alternatives if the utility derived from $j$, denoted $U_{ij}$ is greater than of all other alternatives. This means that the consumer will choose the product with the greatest perceived utility, and adding the same constant to all alternatives will not change the decision. This phenomenon has a particularly important implication: only parameters that capture differences across alternatives can be identified.

\subsubsection{Product-specific constants}

It is often reasonable to include an unobserved product-specific constant in the utility derived from a product $j$ such that $U_{ij} = V_{ij} + \varepsilon_{ij} $, where $$V_{ij} = X_j\beta + \xi_j $$ However, since only differences in utility matter, we will only be able to capture difference between $\xi$s, not their absolute level. To see that consider a binary choice model where $\tilde{\xi_{j}} - \tilde{\xi_{k}} = \xi_j - \xi_k$, then, we would have $$ U_{ij} > U_{ik} \Leftrightarrow \tilde{U_{ij}} > \tilde{U_{ik}} $$ and we would not be able to identify which value of $\xi$ or $\tilde{\xi}$ is the right one. That is why, in a model with $J$ alternatives, at most $J-1$ alternative specific constant can enter the model, and one product must have its $\xi$ normalized to zero.

\subsection{Irrelevance of utility scale}

Just as adding a constant to the utility of all alternatives does
not change the decision maker’s choice, neither does multiplying each
alternative’s utility by a constant, i.e. the product with the highest utility would stay the same. The scale of utility and the variance of the error term are linked by definition, since multiplying $U_{ij}$ by a constant $\lambda$ for all $j$ would imply multiplying the variance of $\varepsilon_{ij}$ by $\lambda^2$ for all $j$. That is why the econometrician will have to normalize the model by normalizing the variance of the error term.

\subsubsection{Normalization with iid errors}

In the case of iid error terms, normalizing the variance is straightforward since normalizing the variance of one error term will normalize the variance of all error terms.

\subsubsection{Normalization with heteroskedastic errors}

In the case of heteroskedastic error terms, when at least one segment of the population/alternatives has a different variance than other segments, normalizing the model has to be done by normalizing the variance of one segment and estimating the variance for all others relative to that segment.

\subsubsection{Normalization with correlated errors}

Finally, when the error term is correlated across products, normalizing for scale is more of an issue, indeed, in that case, normalizing the variance for one product is not enough to normalize the variance of other products. The solution is to set the variance of one of the error differences to a number, then estimating all other variances and covariances relative to the constant.

\section{``Plain-vanilla'' Logit}

\subsection{Choice probabilities}

To derive the logit model, recall the notation used in the previous chapter where each product has a mean quality level $\delta_j$ that incorporates the characteristics of the product as well as its price, and an idiosyncratic error term $\varepsilon_{ij}$. Thus, we have: $$ U_{ij} = \delta_j + \varepsilon_{ij} $$ In the logit model, we assume that the error term is drawn iid, from a Type-I Extreme Value (or Gumbel) distribution. The Gumbel distribution has the following laws of probability: $$ f(x) = e^{-x}e^{-e^{-x}} \text{ and } F(x) = e^{-e^{-x}} $$ Moreover, the variance of the distribution is assumed to be $\pi^2/6$; this assumption is crucial for identification since it implies normalization of the scale of utility (this is explained in the first section of this chapter). The mean of this distribution is not 0 (!) but since only differences between utility levels are important, this fact will not cause any issues. Finally, this distribution assumption allows us to have a closed-form solution for the distribution of the difference between two error terms: specifically, let $\tilde \varepsilon_{ijk} = \varepsilon{ij} - \varepsilon{ik}$, we have that: $$ F(\tilde \varepsilon_{ijk}) = \frac{\exp(\tilde \varepsilon_{ijk})}{1 + \exp(\tilde \varepsilon_{ijk})} $$ This distribution law is almost exactly the same as the Normal distribution, so that even if the extreme-value assumption gives fatter tails and slight asymmetry, we get back a Normal distribution in the differences. Nevertheless, note that the shape of the distribution is not that important and that the more important assumption is independence of errors, which requires a well-specified model to be justifiable (i.e. no missing variables, etc.). The independence assumption also requires that the unobserved part of utility for each product ($\varepsilon_{ij}$) is completely independent with respect to other products! In other words, taste for products cannot be correlated in an unobserved way. If this is the case, then we need to use other models that will be described later.

The probability that consumer $i$ buys good $j$, given $\varepsilon_{ij}$, is the conditional probability that he chooses $j$ over all other $k$: \begin{align*} P_{ij}|\varepsilon_{ij} = \prob{U_{ij}>U_{ik}\text{ for all } j\neq k | \varepsilon_{ij} } & = \prob{\delta_j + \varepsilon_{ij} >\delta_k + \varepsilon_{ik}\text{ for all } j\neq k} \\
& = \prob{ \varepsilon_{ik} < \varepsilon_{ij} + \delta_j - \delta_k \text{ for all } j\neq k } \\
& = \prod_{j\neq k} F(\varepsilon_{ij} + \delta_j - \delta_k) \text{ (by indep.)}
\end{align*}
We can then proceed to compute the unconditional probability by integrating over all possible values of $\varepsilon_{ij}$ from the Gumbel distribution: $$ P_{ij} = \int \left(\prod_{k\neq j} F(\varepsilon_{ij} + \delta_j - \delta_k)\right) f(\varepsilon_{ij})\D\varepsilon_{ij} = \frac{ e^{\delta_{j}} }{ \sum_{k=0}^{J} e^{\delta_{k}} } $$ which is called the logit choice probability, and also equivalent to the market share of product $j$, under the assumption that all consumers are identical in their distributions.

\subsubsection{Some properties}

This model is very practical in the sense it is very easy to set up and displays several desirable properties:\begin{itemize}
\item $s_j = P_{ij}\in(0,1)$ so that any market share can be rationalized by the model, except in extreme cases with only one product having all the sales (in those cases we would not want a logit model anyway), or some products having no sales (then we should take them out of the sample).
\item As $\delta_{j}$ increases, reflecting higher attributed quality, the market share will increase to 1. If $\delta_j$ decreases, then the market share goes to 0. However, note that these results only hold at the limit, meaning a good will never have a market share equal to 0 or 1. If that is the case in the data, the researcher should take the product out of the dataset. This is why estimating a logit model must be done on a time-period long enough so that all goods considered have been sold at least once (think about vending machines).
\item $\sum_j s_j = 1$, meaning that one alternative will be chosen, always.
\item $s_j = P_{ij}$ is everywhere continuously differentiable in the characteristics $\delta_{j}$ and in price $p_j$.
\end{itemize}

The last point is useful to compute price derivatives (and elasticities): \begin{align*}
\frac{\partial s_j(\theta)}{\partial p_j} = \frac{-\alpha e^\delta_j \cdot (1 + \sum_{k}e^\delta_k ) + \alpha e^\delta_j e^\delta_j }{(1 + \sum_{k} e^\delta_k )^2} & = \frac{-\alpha e^\delta_j\cdot\left[ 1 + \sum_{k}e^\delta_j - e^\delta_j\right]}{(1 + \sum_{k}e^\delta_j)^2} \\ 
& = - \alpha \cdot \frac{e^\delta_j}{1 + \sum_{k}e^\delta_j} \cdot \frac{1 + \sum_{k}e^\delta_k - e^\delta_j}{1 + \sum_{k}e^\delta_k} \\
& = - \alpha s_j (1 - s_j)
\end{align*} And using the same method, we find $\partial s_j(\theta)/\partial p_k = \alpha s_j s_k $

\subsubsection{Characterizing mean utility}

The estimation of the mean quality characteristic $\delta_j$ can be done over a set of non-price characteristics $X_j$. In particular, consider the following:
$$ \delta_j = X_j\beta - \alpha p_j + \xi_j $$ where $\alpha$ and $\beta$ are taste parameters for the given characteristics and $\xi_j$ is an independent, unobserved mean utility shock. This shock is important in order to rationalize any pattern of market shares (suppose a product has better characteristics in every way but still has lower shares).

As introduced in the first section of this chapter, product-specific constants can only be estimated by normalizing the $\xi$ of one product to a constant. Here, the choice is already made since the outside good has a zero utility by assumption. However, another issue arises by introducing this term. Iif consumers know $\xi_j$, firms must also know it and price their products accordingly. This introduces endogeneity in the error term with respect to prices. Thus, the econometrician needs instruments to correct for that endogeneity. Typically, these instruments include cost shifters that would not affect intrinsic demand for the good but would definitely have an effect on prices. Note that any non-price characteristic included in $X_j$ that would also be correlated with the unobservable characteristics $\xi_j$ needs to be instrumented as well.

\subsection{Logit and taste variation}\label{sssec:tastevar}

The logit model is very efficient and easy to set up if you need to measure systematic taste variation, that is, variation in tastes associated with observable characteristics $X_j$. Variations associated with idiosyncratic randomness, $\varepsilon_{ij}$ are assumed away as type-I extreme value random variables.

In reality, the value that agents put on a particular attribute varies across these individuals. Sometimes these tastes vary in an identifiable way, for example, low-income agents might more concerned about the price than others; sometimes you might observe people with the exact same observable characteristics choosing different options. Logit models allow for identification of these taste variations only within limits: if this variation is linked with observable characteristics, then logit models can be applied; if this variation is purely random, then we will require other models.

As an example, consider the choice of households between cars, where two characteristics enter the decision: price $p_j$ and shoulder room $x_j$. A household $i$ places value $U_{ij}$ on buying good $j$ such that: $$U_{ij} = \beta_i x_j - \alpha_i p_j + \varepsilon_{ij} $$ where you can see that the parameters $\alpha, \beta$ vary across households $i$. Say the shoulder room taste $\beta_i$ depends only on the number of members in the household $m_i$ such that $\beta_i = \rho M_i$; this means that $m_i$ is positively correlated with $\beta_i$. Similarly let's say the importance of price is negatively correlated to income $I$ so that $\alpha_i = \theta/I_i$. Substituting back into the utility function we get: $$U_{ij} = \rho M_i x_j - \theta p_j/I_i + \varepsilon_{ij} $$ which can be estimated with a standard logit model where variables are interaction between characteristics of the product and characteristics of the household.

In contrast, suppose taste variation was subject to an error term such that $\beta_i = \rho M_i + \mu_i$ where $\mu_i$ is non-observable (a random variable). Then the utility would be written as: $$U_{ij} = \rho M_i x_j - \theta p_j/I_i + \varepsilon_{ij} + \mu_ix_j $$ and the logit would confound the error term with $\tilde\varepsilon_{ij} = \varepsilon_{ij} + \mu_ix_j$ which is clearly correlated across product specifications and households.

Bottomline is logit models can handle systematic taste variation but not random taste variation. For the latter, we will need more complex models, such as the BLP model studied in the next chapter.

\subsection{Derivatives and Elasticities}

The logit model presented above displays features that are not desired in the context of elasticities. These problems imply that one would run into issues while trying to expand logit results in terms of welfare analysis, antitrust analysis, etc.

First, recall the own- and cross-price derivatives and elasticities:
$$ \frac{\partial s_j}{\partial p_j}  = -\alpha s_j (1 - s_j) ; \quad \frac{\partial s_j}{\partial p_k}  = \alpha s_j s_k ; \quad \epsilon_{jj}  = -\alpha p_j (1 - s_j) ; \quad \epsilon_{jk}  = \alpha p_k s_k $$

These imply that:
\begin{itemize}

\item Two products with the same market shares will have the same markups. Indeed, from the first-order conditions, we have that: $$p_j - mc_j = \frac{1}{\vert\epsilon_{jj}\vert} \Leftrightarrow \frac{p_j - mc_j}{p_j} = \frac{1}{\alpha(1 - s_j)} $$ which would be the same for $s_j = s_k$. This is obviously not intuitive but also not observed in reality. 

\item Own-price elasticities are higher for higher priced goods. This fact comes directly from the formula of the own-price elasticity that shows a positive effect (in absolute value) from the prices. This is counter-intuitive since it would imply that people buying higher-priced items would be more price-sensitive than people buying lower-priced goods.

\item Substitution between goods only depends on relative shares and not proximity of product characteristics. In fact, cross-price elasticities are given by $\frac{\partial s_j}{\partial p_k}\frac{p_k}{s_j} = \alpha p_k s_k $, which is not a function of characteristics of neither products.

\end{itemize}

\subsection{Independence of Irrelevant Alternatives (IIA)}\label{sssec:logitiia}

We have seen in the overview of the logit model that idiosyncratic errors are independently distributed across products, following a type-I extreme value distribution. The choice of the distribution turns out not to be very important, but the independence property has some implications that the researcher should know about. In fact, the independence assumption means that the unobserved utility derived from one good ($\varepsilon_{ij}$) is unrelated unconditionally to the unobserved utility from another good. This assumption seems to be rather restrictive in the context of goods but helped us a lot in coming up with the solution of the logit model.

The independence of the error terms turns out to have problematic implications in the realism of the choice mechanism. In fact, suppose a consumer has a choice of going to work by car ($c$) or taking a blue bus ($bb$). Say that utility derived from both models are the same, such that $P_c = P_{bb} = 1/2$, meaning the ratio of probabilities is one. Now suppose that a red bus ($rb$) is introduced in the market such that it is identical in every aspect except the color to the blue bus. The probability of taking the red bus should therefore be the same as taking the blue bus: $P_{rb}/P_{bb} = 1$. However, the ratio of probabilities between the car and the blue bus has not changed because of independence of irrelevant alternatives, meaning that $P_{bb}/P_{c} = 1$, thus $P_c = P_{bb} = P_{rb} = 1/3$ is the prediction of the logit model. In real life though, we would expect the probability to take the car to remain exactly the same since the actual problem is to either take the bus (regardless of the color) or the car, yielding $P_c = 1/2, P_{bb}=P_{rb} = 1/4 $.

This IIA problem is nonetheless also a feature of the logit model when it corresponds to reality. In fact, this property allow the researcher to consider only subsets of the complete set of alternatives and still get consistent estimates, as long as for each observation, the actual choice is kept in the set. Another practical use of that property is that if the researcher is only interested in a few choices, then they do not need to include the other choices in the dataset, leaving the data research part out of the picture.

\subsection{Consumer Surplus}

For policy analysis, the researcher is often interested in measuring the change in consumer surplus that is associated with a particular event (introduction of a product, merger, etc.). Under the logit assumption, this value of the consumer surplus also takes a simple closed form that can be easily computed from the model. We know that each consumer will choose the product that yields the highest utility. In the aggregate case, all consumers have the same tastes so that in expectation (the econometrician does not observe $\varepsilon_{ij}$), consumer surplus is defined as the value of utility derived from the best good. Formally, $$\E{\operatorname{CS}} = \E{\max_{j} \{U_{ij}\}} = \E{\max_{j} \{\delta_{j} + \varepsilon_{ij} \}}$$ which yields: $$\E{\operatorname{CS}} = \ln\left(\sum_{j=1}^{J} e^{\delta_j} \right) + C $$ where $C$ is an unknown that represents the fact that absolute utility cannot be measured. This value is called the logit inclusive value, or the log-sum term. As you can see, comparing two policies is easy in this setting, let one policy be denoted by the superscript $0$ and the other by the superscript $1$. Then, $$\Delta\E{CS} = \ln\left(\sum_{\mathcal{J}^1} e^{\delta_j^1} \right) - \ln\left(\sum_{\mathcal{J}^0} e^{\delta_j^0} \right) $$

If we had transaction-level data with individual characteristics, we could perform this analysis of consumer surplus at each unit of observation and aggregate to find our results.

\section{Nested Logit}

A natural extension of the simple logit model, allowing for richer substitution patterns and a somewhat less restrictive IIA property is the nested logit model.

A nested logit model partitions the choice set in different subsets called nests such that the actual choice of one good follows from choices among nests. For example, in the simplest nested logit, with one nest, consumers first choose a nest (a category of products) as modelled in a simple logit, then within a nest, they choose a product inside the nest (again modelled as a simple logit). This sequence of logit models creates a different type of model called the nested logit. These nests are chosen by the researcher (which is not ideal) and they provide a strong structure to the model which could affect results significantly. Following the notation of Cardell (1991) and Berry (1994), we model utility as: $$U_{ij} = \delta_j + \zeta_{ig} + (1 - \sigma)\varepsilon_{ij} $$ where the new terms are: $\zeta_{ig}$ an idiosyncratic ``nest'' taste shock that applies to all goods $j$ in the nest $g$ ; and $\sigma$ a parameter of correlation in tastes within nests (if $\sigma$ is high, tastes within group are very correlated and the nest structure matters, if $\sigma$ is low, then the correlation in tastes within the nest is small and the nest structure is irrelevant). At first, it might seem that the $\sigma$ term will not allow for the simple logit model, but in fact, the $\zeta_{ig}$ error term follows a unique distribution distribution function such that $\zeta_{ig} + (1 - \sigma)\varepsilon_{ij}$ follows an extreme-value distribution (not type-I however, we call it Generalized Extreme Value or GEV). This makes $\sigma$ important to both terms, and in particular, when $\sigma \to 0$ we go to the simple logit, and $\sigma\to 1$ yields to more within-group correlation.

We know that within the same nest, the utility level  $u_{ij} = \delta_j + \zeta_{ig} (1 - \sigma) + \varepsilon_{ij}$, can be reduced down to ignore the effect of $\zeta$, since it is the same across products. We end up in the same setting as the simple logit model. This implies that the conditional share of product $j$, or the share within the nest, is given by the same formula as the simple logit: $$s_{j\vert g} = \frac{\exp(\delta_j/(1-\sigma))}{\sum_{k\in\mathcal{J}_g} \exp(\delta_k/(1-\sigma))} $$ where the denominator for this expression can be written as $D_g$, the total demand for products in the group $g$.

Meanwhile, across groups, we have that both error terms still give a type-I EV, so again, we can think about the demand for a group as we did in the logit case: $$s_g = \frac{D_g^{(1-\sigma)}}{\sum_{h} D_h^{(1-\sigma)}} $$ Finally, the share of a product $j$ is given by the product of both the share of the group containing $j$ and the conditional share of $j$ within the group: $$s_j = s_{j\vert g} \cdot s_g = \frac{\exp(\delta_j/(1-\sigma))}{D_g^\sigma\cdot\left(\sum_{h} D_h^{(1-\sigma)}\right)} $$ As we can see, demand for product $j$ depends on its own quality level relative to its group, the quality of the group relative to the other groups and $\sigma$, the correlation in tastes within nests.

The outside good in this model is considered as one of its own group, so that $s_{0\vert 0} = 1$ and with the normalization of $\delta_0 = 0$ and $D_0 = 1$, we get:$$s_{0} = 1\cdot s_0 = \frac{1}{\sum_h D_h^{(1-\sigma)}} $$

This analytical derivation is the same when we extend the model to have nests within nests and more.

\subsection{IIA and substitution patterns}

The nest structure of this model satisfies two properties:\begin{enumerate}

\item For any two alternatives that are in the same nest, the ratio of probabilities is independent of the attributes and/or existence of other products in the nest or in other nests. $$ s_j / s_k = s_{j\vert g} / s_{k\vert g} = \frac{\exp(\delta_j/(1-\sigma))}{\exp(\delta_k/(1-\sigma))} $$ Equivalently, we say that IIA holds within the nest.

\item For any two alternatives that are in different nests, the ratio of probabilities depends on the attributes and/or existence of other products in the two nests. $$ s_j / s_k = s_{j\vert g} / s_{k\vert h} \cdot s_g / s_h  = \frac{\exp(\delta_j/(1-\sigma))}{\exp(\delta_k/(1-\sigma))} \cdot \frac{D_g^{1-\sigma}}{D_h^{1-\sigma}} $$ Equivalently, we say that IIA does not hold across nests.
\end{enumerate}

\subsection{Research considerations}

As we have seen, the nested logit is a fairly simple extension on the simple logit case, which relies on relaxation of the IIA property across groups (or nests). The model is often told in a narrative of sequential choice: consumers first choose a group $g$ ($s_g$), then a product (or group) $j$ within group $g$ ($s_{j\vert g}$). 

The results of the model depend very strongly on the ex ante nesting structure chosen by the researcher. Therefore, it is very important (and hard) to understand what the appropriate structure should be. The sequential narrative is supposed to help the researcher come up with an accurate structure but it might be unhelpful at times.

Identification comes in different flavors:\begin{itemize}
\item Parameters associated with characteristics and prices are identified within group by variation in these exact characteristics.
\item The correlation in tastes parameter ($\sigma$) is identified by variation in 
\end{itemize}

\section{Multinomial Probit}

\subsection{Intuition}

The two previous models covered in this chapter had two main issues: they cannot handle random taste variation and they imply restrictive substitution patterns due to IIA (even though the nested logit solves this issue in some way). The multinomial probit on the other side, is not affected by these two issues at all. In essence, the MNP approach allows for correlation between choices thanks to a flexible covariance matrix of the errors.

\subsection{Choice probabilities}

As in the general case, we start with writing the individual (unconditional) probability of choosing product $j$: \begin{align*} P_{ij} & = \E{\prob{U_{ij}>U_{ik}\text{ for all } j\neq k | \varepsilon_{ij} }} \\ & = \E{\prob{\delta_j + \varepsilon_{ij} >\delta_k + \varepsilon_{ik}\text{ for all } j\neq k| \varepsilon_{ij}}} \\
& =\E{ \prob{ \varepsilon_{ik} < \varepsilon_{ij} + \delta_j - \delta_k \text{ for all } j\neq k| \varepsilon_{ij} }} \\
& = \int \prod_{j\neq k} F(\varepsilon_{ij} + \delta_j - \delta_k) f(\varepsilon_{ij})\D \varepsilon_{ij}
\end{align*}
But in the multinomial probit case, the law of probability for $\varepsilon_{ij}$ depends on all the other error terms! Thus we have correlation between choices, that we estimate.

Formally, we have: $$ \varepsilon_i \sim N(\mu, \Omega) $$ where $\varepsilon_i$ is the vector of all product error terms. This means that $\Omega$ is a $(J+1)^2$-elements symmetric matrix. In practice, we restrict $\mu$ to be 0 (especially if we estimate $\xi$s), but following the first section of this chapter we will also need to restrict the variance of the error terms heavily. In fact, as mentioned in that section, in the case of iid but correlated error terms, we need to normalize one diagonal term of the error-differences distribution and estimate the other terms relative to that term.

\subsection{Applications}

The MNP approach is interesting in most cases where choice correlations are important, but is limited by the fact that data requirements get huge when we increase the number of products.

\section{Estimation Strategy for Product-level data}

The general estimation strategy that applies to logit models with product-level data is detailed in the following steps:\begin{enumerate}
\item Assume that data is drawn from markets with large $n$.
\item Assume that observed market shares are measured without errors.
\item For each $\theta$ (vector of parameters), there exists a unique $\xi$ such that the model shares and observed shares are equal ($J$ equations, $J$ unknowns).
\item We invert the model to get $\xi$ as a function of the parameters. How this step is performed depends on the functional form of the model.
\item Using $\xi$, we can create the moments of the model, estimating them by GMM. 
\end{enumerate}

\subsection{Inversion in the logit case}

We need to express the quality unobservable term $\xi_j$ in terms of all observable characteristics. First, recall that: $$ \delta_j = X_j\beta - \alpha p_j + \xi_j $$ but in this equation, $\delta_j$ is unknown (there is no such thing as a perceived mean quality level). Nevertheless, we can use the market shares formulas that link the $\delta_j$ to the observed market shares $s_j$ (observed without errors). Thus, $$s_j = \frac{\exp(\delta_j)}{1 + \sum_k \exp(\delta_k) } $$ but again, we run into a problem since it is expressed as a function of other $\delta_k$: we could solve a system of equations, or simplify the model using the normalized good. Indeed, $$\frac{s_j}{s_0} = \exp(\delta_j) \Leftrightarrow \ln(s_j) - \ln(s_0) = \delta_j \Leftrightarrow \ln(s_j) - \ln(s_0) = X_j\beta - \alpha p_j + \xi_j $$ which in turn yields the inversion equation: $$
\ln(s_j) - \ln(s_0) +  \alpha p_j  - X_j\beta = \xi_j $$

\subsection{Inversion in the nested-logit case}

In the same way as in the simple logit model, our main objective is to get the link between $\delta_j$ and observables so that we can identify $\xi_j$. In this case: $$s_j = \frac{\exp(\delta_j/(1-\sigma))}{D_g^\sigma\cdot\left(\sum_{h} D_h^{(1-\sigma)}\right)} \text{ and } s_0 = \frac{1}{\sum_{h} D_h^{(1-\sigma)}} $$ so that $$\ln(s_j) - \ln(s_0) = \frac{1}{1-\sigma}\cdot \delta_j - \sigma\ln(D_g) $$ We turn into a new problem caused by the two-level structure, which is $D_g$ is not parameterized. Again, we solve this issue by using the normalized good: $$ s_g / s_0 = D_g^{(1-\sigma)} \Leftrightarrow \ln(s_g) - \ln(s_0) = (1-\sigma) \cdot \ln(D_g) $$ which we can plug back in the previous equation to get: $$\ln(s_j) - \ln(s_0) = \frac{1}{1-\sigma}\cdot \delta_j - \frac{\sigma}{1 - \sigma} [ \ln(s_g) - \ln(s_0) ] $$ $$\Leftrightarrow (1 - \sigma) \cdot [\ln(s_j) - \ln(s_0)] = \delta_j - \sigma [ \ln(s_g) - \ln(s_0) ] $$ $$\Leftrightarrow (1 - \sigma) \cdot \ln(s_j) + \sigma \ln(s_g) - \ln(s_0) = \delta_j $$ Finally, using the fact that $s_g = s_j / s_{j\vert g} \Leftrightarrow \ln(s_g) = \ln(s_j) - \ln(s_{j\vert g})$, we can get: $$\Leftrightarrow \ln(s_j) - \sigma \ln(s_{j\vert g}) - \ln(s_0) - X_j\beta + \alpha p_j = \xi_j $$

\subsection{Endogeneity issues}

\subsubsection{Simultaneity}

Regardless of the model we use, we will have to deal with endogeneity issues regarding many dimensions, such as prices, observable characteristics or market shares. 

In fact, in both models price was correlated to the unobservable term as firms make their decision based on demand which includes this term; moreover, in the nested logit case, we also have to deal with endogeneity in the within-share term. In general, anything that is correlated with the unobservable quality term $\xi$ will have to be instrumented.

\subsubsection{Measurement errors}

Measurement errors in observed prices, characteristics or quantities may also create difficulties for the estimation procedure outlined above. Prices enter linearly in the estimation and are already endogenous so that these errors do not create too important biases. However, when these errors are present in market shares or quantities data then this issue becomes more important. Indeed, market share data is used to invert the model and get the unobservable term as a function of observed data. If shares include measurement errors, the non-linear inversion will accentuate the errors in a non-tractable way, creating a lot of issues in estimation. In practice, maximum likelihood will not have this kind of issue, but will make IV strategies harder.

\subsubsection{Instruments}

For identification, we need to satisfy orthogonality conditions between $\xi_j$ and the covariates (product characteristics $x_j$ and price $p_j$). As discussed earlier, price will for sure be endogenous, so we need at least one instrument for this. Formally, we need a set of instruments $w$ such that $\E{\xi|x, w} = 0$.  In practice, we consider three types of instruments:\begin{itemize}
\item Cost shifters: they are the ideal instruments but are not always observed.
\item Hausman instruments: they consist of prices of the same product in other markets, however, they might not be valid when the same shock affects all markets together.
\item BLP instruments: they are the competing products' characteristics. This approach is purely based on heuristics (no model) and is usually combined with other instruments to reduce the dimensionality requirement of other more valid instrument.
\end{itemize}

\section{Estimating the supply side}\label{sec:supplyside}

We may also be interested in estimating a supply side, either on its own for counterfactual estimation or as a mean to improve efficiency of demand parameter estimation. In order to do this, we need two assumptions: one on the cost function that firms face, and another on the type of competition in the market.

Consider the following example. We start with a linear marginal cost function: $$\ln(mc_{jt}) = w_{jt}\gamma + \omega_{jt} $$ where $w_{jt}$ are the observed cost shifters and $\omega_{jt}$ the unobserved ones. Then, using the competition assumption, we recover the first-order conditions vector which has the following general form: $$ p_t - mc_t = - (\Omega \star \Delta)^{-1} s_t $$ where $\Omega$ is the ownership matrix and $\Delta$ the $J\times J$ elasticity matrix. Note also that the $\star$ operator represents element-wise multiplication.

We can clearly see why demand estimation is important in that case: elasticities! Without the elasticities it is impossible to recover the marginal cost from the FOCs. Now, we can rewrite the equation as: \begin{align*}
mc_t  & = p_t + (\Omega \star \Delta)^{-1} s_t \\
 \Leftrightarrow \ln(mc_t) & = \ln(p_t + (\Omega \star \Delta)^{-1} s_t) \\ \Leftrightarrow \omega_{jt} & = \ln(p_t + (\Omega \star \Delta)^{-1} s_t) - w_{jt}\gamma
\end{align*} 
And using the same instruments as in our demand estimation procedure, we should have that $\E{\omega|x, w} = 0$. Then, we collect all demand and supply moments and estimate by GMM simultaneously.

\section{Goldberg (1995)}

\subsubsection{Background}



\subsubsection{Model}



\subsubsection{Data}



\subsubsection{Assumptions}



\subsubsection{Results}

