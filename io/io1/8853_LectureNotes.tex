\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{libertine}
\usepackage{amsmath,amsthm}
\usepackage[varg]{newtxmath}
\usepackage{setspace,graphicx,epstopdf}
\usepackage{marginnote,datetime,url,enumitem,subfigure,rotating}
\usepackage{multirow}
\usepackage{xfrac}
\usepackage[openlevel=3]{bookmark}
\usepackage[tikz]{bclogo}
\usepackage{enumitem}

\setcounter{tocdepth}{2}

\def\D{\mathrm{d}}

\setlength{\parindent}{0ex}
\setlength{\parskip}{1em}%Espacement des par

\setlist[itemize]{topsep=-6pt, itemsep=-1pt}
\setlist[enumerate]{topsep=-6pt, itemsep=-1pt}

\newcommand{\smalltodo}[2][] {\todo[caption={#2}, size=\scriptsize,%
fancyline, #1]{\begin{spacing}{.5}#2\end{spacing}}}
\newcommand{\rhs}[2][]{\smalltodo[color=green!30,#1]{{\bf PS:} #2}}

\newcommand{\E}[1]{\operatorname{E}\left[#1\right]}
\newcommand{\Et}[1]{\operatorname{E}_t\left[#1\right]}
\newcommand{\V}[1]{\operatorname{Var}\left[#1\right]}
\newcommand{\cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\covt}[1]{\operatorname{Cov}_t\left(#1\right)}
\newcommand{\avg}[2]{\frac{#1}{#2} \sum_{i=#1}^{#2}}
\def\D{\mathrm{d}}
\newcommand{\prob}[1]{\operatorname{Pr}\left[#1\right]}


\begin{document}

\date{}
\title{\textbf{\huge{ECON8853 - Industrial Organization I}}\\ \textit{Lecture Notes from Julie H. Mortimer's lectures}}
\author{Paul Anthony Sarkis\\ Boston College} 
 
\maketitle

\tableofcontents

\chapter*{Undergraduate IO Review}

\section{Monopoly and Imperfect competition with homogeneous and differentiated products}



\section{Oligopoly with homogeneous products}



\section{Differentiated products}



\chapter{Introduction to Empirical Industrial Organization}



\section{Generic Static Analysis}

Static analysis refers to the study of firm behavior and market structure in a non-dynamic context. This means that all analysis is conditional on current:\begin{itemize}
\item Goods marketed and their cost functions: investment in output generating units is not condisered, nor is R\& D, or any other "dynamic elements".
\item Consumer preferences: any behavior that might affect changes in preferences is not studied
\item Institutional features: changes in the regulation context or market rules are not studied either.
\end{itemize} A comparative study will thus involve changing one of these conditions and study the effect on market outcomes by comparison with actual outcomes.

A typical model of static analysis in IO involves three main components:\begin{enumerate}
\item a demand system
\item a cost system
\item an equilibrium assumption
\end{enumerate} Then, (1) and (2) will be estimated or recovered from actual data, while usually (3) will depend on assumptions made by the economist following knowledge about the institutional setting in which the transactions take place. Generally, economists are very interested in "testing" the assumptions made in (3).

The first class of models that we will cover makes the (strong) simplifying assumption that current decisions of the firms have no effect on future outcomes. This corresponds to a view of markets in which firms are very myopic and do not consider future implications of their choices.

\subsection{Aside on Dynamic Effects}

In comparison with static analysis, dynamic models use state variables that are affected by current and past outcomes and will affect future outcomes. While static analysis recovers immediate impacts, dynamic analysis allows for feedback effects, etc. so that analysis is provided on a longer term.

We will eventually study this type of analysis, but we will cover it following intermediate steps such as two-period models, single-agent dynamic models and then full-fledged dynamic models.

\subsection{Aside on Theoretical IO and Econometrics}

Theoretical advances are not the main drivers of IO research these days. Nevertheless, they still provide essential notions for analysis such as notions of equilibria for different environments and/or institutional settings. This means that a good IO economist should always be able to understand, critique and defend the theoretical models used in the literature or their own work.

A good resource for reviewing modern (yet not complex) IO models is Oz Shy's textbook: "IO: Theory and Applications".

Econometrics provides us the tools of statistic analysis of the features in our models. The econometric side of an IO paper should always include simple descriptive statistics that show a simple correlation to demonstrate a phenomenon, then a complete analysis of the model used in the research.

\section{Review of the Cournot model}

Suppose the demand system is given by a set of CES demand functions for $N$ different markets, such that: $$\ln(Q_n) = \beta x_n - \alpha \ln(p_n) + \varepsilon_n $$ where $n$ indexes markets, $Q_n$ is the total quantity sold, $x_n$ is a set of observed market characteristics and $\varepsilon_n$ is a set of unobserved market characteristics.

In a Cournot model, firms compete over quantities (Nash-in-quantities). This means that we need to transform the demand curve to an inverse demand curve giving the price for a certain quantity: $p_n(Q_n)$ where $Q_n = \sum_{j} q_j$ is the sum of all firms' output. We denote firms with the index $j$ and define their cost function as $C_j(\cdot)$ while their marginal cost is $mc_j$. Firms can have different cost functions and/or different marginal costs.

Thus, the profit function of a single firm (assuming a single market)is: $$\pi_j(q_j) \equiv p(Q)q_j - C_j(q_j)$$ Assuming full differentiability and strictly positive quantities we get the first-order conditions as: $$ p(Q) + \frac{\partial Q}{\partial q_j}p'(Q)\cdot q_j - mc_j(q_j) = 0 $$ In the Cournot setting, firms take the other firms' decision as given, so that they see $\frac{\partial Q}{\partial q_j}$ as equal to 1. This yields:$$p(Q) + p'(Q)\cdot q_j - mc_j(q_j) = 0 $$ and the second-order condition: $$2 p'(Q) + p''(Q)\cdot q_j - mc_j'(q_j) $$ which is strictly negative if and only if: 

\section{Review of the Bertrand model}



\section{Comments on equilibrium notions}



\section{Strategic complementarity of diff. products}



\chapter{Introduction to Demand Estimation}

Recall the multi-product Bertand model (also found in problem set 0) where a firm $f$ chooses the price vector for its set of products $\mathcal{J}_f$ such that it maximizes its profits. Formally, $$\max_{\{p_j\}_{j\in \mathcal{J}_f}} \pi_f(\mathbf{p}) = \sum_{j\in\mathcal{J}_f} (p_j - c_j)\cdot q_j(\mathbf{p}) $$ where the boldface typed $\mathbf{p}$ represents the vector of all available products (not only those sold by the firm). Taking the FOCs, we get, for each $j\in\mathcal{J}_f$: $$ q_j(\mathbf{p}) + \sum_{k\in\mathcal{J}_f} (p_k - c_k)\cdot \frac{\partial q_k(\mathbf{p})}{\partial p_j} = 0 $$ Define the matrix $\Omega$ as follows: $$\Omega_{(j, k)}(\mathbf{p}) = \begin{cases}
-\frac{\partial q_k(\mathbf{p})}{\partial p_j} & \text{ if } j, k \in \mathcal{J}_f \\
0 & \text{ else.}
\end{cases} $$ Then we can rewrite the set of FOCs for all products in $\mathcal{J}_f$ as: $$ q(\mathbf{p}) = \Omega(\mathbf{p})\cdot(\mathbf{p} - \mathbf{mc}) $$ This formula is the most important formula of demand estimation and thus must be understood deeply. To help with understanding it, consider a simple example: there are only $J = 3$ products on the market, of which firm 1 sells only the first two. Its two FOCs are: $$q_1(\mathbf{p}) = -\frac{\partial q_1(\mathbf{p})}{\partial p_1} (p_1 - c_1) - \frac{\partial q_2(\mathbf{p})}{\partial p_1} (p_2 - c_2) - 0 $$
$$q_2(\mathbf{p}) = -\frac{\partial q_1(\mathbf{p})}{\partial p_2} (p_1 - c_1) - \frac{\partial q_2(\mathbf{p})}{\partial p_2} (p_2 - c_2) - 0 $$ or in matrix form: $$\begin{bmatrix}
q_1(\mathbf{p}) \\
q_2(\mathbf{p})
\end{bmatrix} = 
\begin{bmatrix}
-\frac{\partial q_1(\mathbf{p})}{\partial p_1} &  - \frac{\partial q_2(\mathbf{p})}{\partial p_1} \\
-\frac{\partial q_1(\mathbf{p})}{\partial p_2} &  - \frac{\partial q_2(\mathbf{p})}{\partial p_2}
\end{bmatrix} \cdot \left(
\begin{bmatrix}
p_1 \\ p_2
\end{bmatrix} - 
\begin{bmatrix}
c_1 \\ c_2
\end{bmatrix} \right) $$ but we miss the third product, so in order to include it we add a line. The final matrix form is given by: $$\begin{bmatrix}
q_1(\mathbf{p}) \\
q_2(\mathbf{p}) \\ 0 
\end{bmatrix} = 
\begin{bmatrix}
-\frac{\partial q_1(\mathbf{p})}{\partial p_1} &  - \frac{\partial q_2(\mathbf{p})}{\partial p_1} & 0 \\
-\frac{\partial q_1(\mathbf{p})}{\partial p_2} &  - \frac{\partial q_2(\mathbf{p})}{\partial p_2} & 0 \\
0 & 0 & 0
\end{bmatrix} \cdot \left(
\begin{bmatrix}
p_1 \\ p_2 \\ p_3
\end{bmatrix} - 
\begin{bmatrix}
c_1 \\ c_2 \\ c_3
\end{bmatrix} \right) $$ $$\Leftrightarrow q(\mathbf{p}) = \Omega(\mathbf{p})\cdot(\mathbf{p} - \mathbf{mc}) $$
As stated before, this equation is the most important equation of demand systems. Once we estimate (or assume partly) predicted sales $q(p)$ and elasticities $\Omega$, we can recover markups and marginal costs. This will be particularly useful for recovering data that we do not have and completing further analysis of the market such as: comparative statics, welfare impacts, research on advertising, research on elasticities, evaluation of new products, etc.

Nevertheless, demand estimation is not that easy. In fact, a demand system incorporates two major challenges that make it particularly hard to estimate: the endogeneity of prices and the flexibility need for multiproduct demand.

Endogeneity of prices arises because prices are not randomly assigned to quantity levels, but they are set as a result of a strategic decision which typically include quantity considerations. This means that there exists simultaneity in the process of coming up with equilibrium prices and quantities. Different market outcomes could be linked to either demand shifts, supply shifts or both, without the econometrician observing it. This issue is mainly addressed by using instruments.

The flexibility issue is linked to the dimensionality of the matrix $\Omega$. As the number of products $J$ increases, the number of own- and cross-price elasticities increase quadratically (there are $J^2$ elasticities to estimate). Moreover it might also be the case that the elasticities matrix varies with $p$ in a way that is not captured by the model (think about $\partial \mathbf{p}/\partial p_j$). Finally, one could consider what can be said of second-order derivatives or third-order derivatives, increasing the dimensionality even more.

There are many types of demand models, but most of them can be separated in these categories:\begin{itemize}
\item Representative vs. Heterogenous consumers?
\item Discrete vs. continous choice?
\item Single vs. multiple products?
\begin{itemize}
\item Within the multiple products case: products vs. characteristic space?
\end{itemize}
\end{itemize}

The data used in these types of models are often aggregate, market-level data on the pairs ($p_j, q_j$). Sometimes it contains more details to it such as the characteristics of the goods, the characteristics of the individuals buying the goods (either aggregate or micro-data), the environment around, etc.

\section{An early example: Bresnahan (1987)}

Bresnahan (1987) is one of the first empirical assessment of competition using demand estimation, spurring the birth of what is now called NEIO (New Empirical IO). The paper studies the presence of collusion in the automobile industry by looking at a dramatic price decrease event that happened in 1955.

The intuition of this paper is quite simple: assuming marginal costs stayed relatively constant during that period, Bresnahan estimates the variation in demand elasticities and compares it to a change in competition model. Using data on 85 models over 3 years, he finds that a change from a collusive to a more competitive equilibrium is consistent with the estimated change in elasticities.

\section{Single product demand}

\subsection{Representative agent}

A demand system is generally thought to be a (more or less) general approximation of a true underlying demand curve, for example: $$\ln(q) = \alpha p + X\beta + \varepsilon $$ where the $\varepsilon$ captures any variation that is unobserved by the econometrician. Using that equation, the objective of the econometrician is to recover the underlying parameters $(\alpha, \beta)$ in order to understand the effects of prices and characteristics on demand. Note that this specification assumes a representative consumer choosing a (total) quantity $q$ for a given price $p$. On the other side, the firms that provide the good might also choose their prices $p$ and characteristics $x$ as a function of the expected demand, which is correlated with $q$. If this is the case, we say that we have endogeneity (from simultaneous equations, as we've seen in econometrics). Since the coverage of IV estimation in that case is more of an econometrics topic, we will assume knowledge of this in the rest of the chapter.

\subsection{Heterogeneous agents}

While the previous model is very simple, a straightforward extension would be to allow for heterogeneous agents on the market. To do that, we need to extend the previous model in two ways: (1) use a micro-founded model for individual demand that aggregates nicely and (2) estimate aggregated demand as: $$ \ln(q) = \int \gamma_i g(\gamma)\D\gamma + \int \alpha_i p f(\alpha)\D\alpha + \int \beta_i x h(\beta)\D\beta + \epsilon $$ where $\alpha, \beta$ and $\gamma$ follow known distributions with unknown parameters to be estimated.

\section{Multi-product demand}

When we enter demand systems with multiple products, we need a way to differentiate between them.

\subsection{Product Space approach}

This course explores product space approaches only briefly since most of modern IO revolves around the characteristic space approach.

In product space, the basic breakdown between models is whether they include a representative agent or heterogeneous agents. The most interesting of them both is the heterogeneous agents ones since it uses more of the available information (better estimates) but also provides a framework for distributional policy analysis (estimating the distribution of responses). These models have been around for a long time but they still face big issues such as treatment of zeroes (when some consumers have no access to a particular good) or aggregation issues (computational). This is why much of the work that has been done in product space revolves around representative agent models, starting with the AIDS model by Deaton and Muellbauer (1980).

\subsubsection{Almost Ideal Demand System (AIDS)}

The motivating goal behind the AIDS model is to get a very general, aggregate demand system starting from consumer theory. This demand system is one of the first to exactly aggregate over consumers. This will provide a system that will provide the $\Omega$ matrix that we are looking for, while satisfying the basic axioms of consumer theory. 

Begin by defining a general expenditure function for a given group of (comparable) goods: $$ \ln e(u, p) = (1 - u)\ln a(\mathbf{p}) + u \ln b(\mathbf{b}) $$ where $u$ is given between 0 (subsistence) and 1 (bliss). This means that $a(\mathbf{p})$ is the cost of subsistence and $b(\mathbf{p})$ is the cost of bliss. The expenditure function therefore represents a linearly homogenous function of both costs.

Now, in order to go deeper, we need to specify exactly what are $a(p)$ and $b(p)$. Intuitively, $\ln b(p)$ should include $\ln a(p)$ since it is the cost of going beyond subsistence. In particular we will have: $$\ln a(p) = a_0 + \sum_{k} a_k \ln (p_k) + (1/2)\cdot \sum_{k}\sum_{j} \gamma_{jk}^* \ln (p_k)\ln (p_j) $$ $$\ln b(p) = \ln a(p) + \beta_0 \prod_{k} p_k^{\beta_k} $$ This yields the complete expenditure function as: $$ \ln e(u, p) = a_0 + \sum_{k} a_k \ln (p_k) + (1/2)\cdot \sum_{k}\sum_{j} \gamma_{jk}^* \ln (p_k)\ln (p_j) + u\cdot \beta_0 \prod_{k} p_k^{\beta_k} $$
We estimate $(\alpha_i, \beta_i, \gamma_{jk}^*)$ from the data, usually under the following assumptions:\begin{itemize}
\item $\sum_i a_i = 1$:
\item $\sum_j \beta_j = 0$:
\item $\sum_k \gamma_{jk}^* = 0$:
\item $\gamma_{jk}^* = \gamma_{kj}^*$: 
\end{itemize}

Applying Shepard's Lemma and log differentiation, we can obtain the expenditure share of any good $k$ within the group, denoted $w_k$: $$w_k = a_k + \sum_j \gamma_{kj} \ln p_j + \beta_k \ln (x/P) $$ where $\gamma_{jk}$ is the average of $\gamma_{jk}^*$ and $\gamma_{kj}^*$; $x$ is the total expenditure within the group and $P$ the price index of the group.

There are two ways of computing the price index of the group:\begin{itemize}
\item Directly from the specification:
\item As a linear approximation:
\end{itemize}

\subsection{Issues with AIDS}

All in all, AIDS works very well when products are actually grouped in certain categories. This is the reason why it is widely used in the trade and macro-consumption literatures. It can be used in IO settings where groups could be single products but it requires instrumenting for every good!

Think of the case where there are $J$ goods, then you need to estimate $J^2$ elasticities or at least $J\times (J+1)/2$ if you are assuming that cross-price elasticities are symmetric. Usually, IO economists try to escape the problem by actually grouping the goods, however all the results will be dependent on your initial choice of grouping. Researchers have to think very deeply in the mechanism they use for grouping. Grouping subject to particular characteristics lead you closer to characteristic space models, but even then, how do you group when the characteristics are continuous etc.

Finally, let's look at one of the main challenges with the product space approach. Suppose a new good is introduced in a market, how can you use the previous demand estimated which did not account for the new product? This creates two issues:\begin{enumerate}
\item Equilibrium effects: the introduction of a new good typically has significant effects on the pricing of the previously existing goods.
\item Extrapolation: using the previously estimated demand implies projecting demand where it is not defined.
\end{enumerate}

\subsection{Examples: Hausman, Leonard and Zona (1994)}

The goal of the paper is to estimate demand for beer in the US in order to perform a merger analysis and test assumptions about the firms' conduct.

The market is divided hierarchically in three levels:\begin{enumerate}
\item The top-level is beer against other goods. This is estimated using a log-log expenditure function: $$\ln e_t = \beta_0 + \beta_1 \ln y_t + \beta_2 \ln P_{bt} + Z_t\delta + \varepsilon_3 $$ where $e_t$ is total expenditures, $y_t$ is income and $P_{bt}$ is the beer price index.
\item Within the beer group, the middle-level are the segments: premium, popular and light. Again, a log-log functional form is assumed so that for a given segment $m$, the demand for the segment is: $$\ln q_m = \beta_m \ln e_t + \sum_{m'} \sigma_{m'} \ln \pi_{m'} + \alpha_{m'} + \varepsilon_2 $$
\item Within each segment, the bottom-level contains five brands. This level is assumed to follow an AIDS: $$w_k = a_k + \sum_{j} a_{jk} \ln p_j + \beta_k \ln (x/P) + \varepsilon_1 $$
\end{enumerate}

In the bottom-level, you have to instrument the price since it is correlated with unobserved product quality (taste, etc.) and unobserved demand shocks (special events like the World Cup, etc.). Then you must find brand-level instruments: this is where the infamous Hausman instruments come in. Hausman instruments use prices in one city to instrument for prices in other cities. These instruments are typically strong, however, their relevance is very questionable. In particular, think about nation-wide advertising which was the main criticism of these instruments. They would clearly affect both cities' prices in the same way creating correlation. Most of the time you can observe these patterns but be wary of unobserved shocks that could affect the complete market. Optimally, the best instrument would be observed input costs for the brand, tax rates, etc.

Hausman results:


\subsection{When should you choose product space approach?}



\section{Multi-product demand: Characteristic Space}

The characteristic space approach follows a different philosophy than the product space:\begin{itemize}
\item Products are considered as a vector of characteristics
\item Consumers' preferences (and thus their utility functions) are defined over these characteristics.
\item Consumers are assumed to choose the characteristic vector (the product) that maximize their utility.
\begin{itemize}
\item One consumer makes one choice to buy a single product. Allowing for buying more than one product is computationally costly and is an open area for research.
\end{itemize}
\item Demand is aggregated by simply summing over consumers' choices.
\end{itemize}

\subsection{History of the characteristics approach}



\subsection{Formal base model}

Assume the following context:\begin{itemize}
\item A consumer $i$ is offered $J$ alternatives. Consumer $i$ is identified by his characteristics, $v_i$, similarly to the product being defined by its characteristics $x_j$ and its price $p_j$.
\item He must choose one option only $j\in J$, which he'll do with probability $P_{ij}$
\item Utility of an individual $i$ for a good $j$ is given by: $$U_{i,j} \equiv U(x_j, p_j, v_i, \theta) \text{ for all } j = 0, 1, ..., J $$ where good $j =0$ is referred to as the outside good.
\end{itemize}  The variable (or most probably the vector) $x_j$ contains non-price characteristics about good $j$, for example the size of the engine, the AC system, the dashboard software, etc.; while $v_i$ is the vector of consumer characteristics such as income, age, etc. Finally, $\theta$ is the vector of parameters of the utility which is to be estimated. Note that in order to estimate the model correctly, you need to be sure that all consumers (or groups of consumers) in the sample have the same access to all $J$ goods, or in other words, they face the same choice set.

Consumer $i$ will choose good $j$ if and only if $U_{i,j} > U_{i, k}$ for any other good $k$. This means that the probability of consumer $i$ buying good $j$ is given by: $$\prob{U_{ij}>U_{ik}\text{ for all } k\neq j} $$ Now assuming that utility is given by an observable component $V_{ij}$ and an unobservable component $\varepsilon_{ij}$, we can rewrite the probability of buying the good as: \begin{align*} P_{ij} & = \prob{U_{ij}>U_{ik}\text{ for all } k\neq j} \\
& = \prob{V_{ij}+\varepsilon_{ij}>V_{ik}+\varepsilon_{ik}\text{ for all } k\neq j} \\
& = \prob{\varepsilon_{ij} - \varepsilon_{ik} > V_{ik} - V_{ij} \text{ for all } k\neq j}
\end{align*} Finally, define $f(\varepsilon_i)$ as the $J$-dimensional probability density function for consumer $i$. Then the probability $P_{ij}$ can be written as: $$P_{ij} = \int \mathbb{I}\{\varepsilon_{ij} - \varepsilon_{ik} > V_{ik} - V_{ij}\}f(\varepsilon_i)\partial \varepsilon_i $$ which is (as $\varepsilon_i$) a $J$ dimensional integral over $f(\varepsilon_i)$.

We'll see later on that some choices could make our life easier in estimating this integral (hard to do it analytically), mainly by parameterizing the error term:\begin{itemize}
\item \textbf{Multivariate normal:} $\varepsilon_i \sim N(0, \Omega)\quad \to $ multinomial probit model.
\item \textbf{Type 1 Extreme-value:} $f(\varepsilon_i) = e^{-\varepsilon_{ij}} e^{-e^{-\varepsilon_{ij}}} \quad \to $ multinomial logit model.
\item other variants...
\end{itemize}

We could also turn to thinking about absolute demand for good $j$ instead of its probability, which is given by the size of the set $S_j(\theta)$ defined as: $$ S_j(\theta) \equiv \{i : U_{i,j} > U_{i, k} \text{ for all } k \}$$ Now suppose consumers are distributed following a pdf $f(v\vert \theta)$, we could recover the market share of good $j$ as: $$ s_j(\mathbf{x}, \mathbf{p}\vert \theta) = \int_{i\in S_j(\theta)} f(v)\D v $$ Given a total market size of $M$, total demand for good $j$ is: $M\cdot s_j(\mathbf{x}, \mathbf{p}\vert \theta)$

As we've seen the complete model of characteristic space relies on utility functions and error terms distributions. This is why, unsurprisingly, the functional forms of utility and errors are exactly what will differentiate the models within the characteristic space approach.

\subsection{Types of models}

There are 5 main models of characteristic space.

\subsubsection{Pure horizontal model}

Analog to the Hotelling model (in its simplest form), there are $n$ ice cream sellers along a beach where consumers are distributed. A consumer $i$ has utility: $$U_{ij} = \bar u - p_j + \theta(\delta_j - v_i)^2 $$ where the term $(\delta_j - v_i)^2$ captures some kind of quadratic transportation cost in the distance from $i$'s preferences ($v_i$) to the characteristics of product $j$ ($\delta_j$). In the simple Hotelling model, $\delta_j$ and $v_i$ are location parameters (where ice cream seller $j$ and consumer $i$ are on the beach).

\subsubsection{Pure vertical model}

On the pure vertical, you don't need to model a distance from consumer preferences since everyone agrees on what product is better. In this case, utility is given by: $$ U_{ij} = \bar u - v_ip_j + \delta_j $$ The interaction between consumer characteristics and prices show that although every consumer has the same utility for the characteristics of good $j$ (i.e. $\bar u + \delta_j$), consumers differ in their willingness to pay ($v_i$).

\subsubsection{Logit model}

In the logit model, consumers have the same taste for the goods' characteristics but are subject to an idiosyncratic shock depending on both the product and the consumer. Utility is given by: $$U_{ij} = \delta_j - p_j + \varepsilon_{ij} $$ where $\varepsilon_{ij}$ follows an iid extreme-value type I distribution. Analogous to the OLS, the error term allows for identification of the other parameters as long as it is not correlated with the $\delta_j$ and the price.

\subsubsection{Pure characteristic model}

This model nests both concepts of verticality and horizontality of differentiation.

Utility is given by: $$ U_{ij} = f(v_i, p_j) + \sum_{k}\sum_{r} g(x_{jk}, v_{ir}, \theta_{kr}) $$ so that it could handle vertical model if $g(\cdot)$ is 0 for cross-products variables. It could also handle the horizontal model. It is a very general form and rarely used?

\subsubsection{BLP model}

The BLP model (for the authors' names Berry, Levinsohn and Pakes), is a parameterized version of the pure characteristics model. It is probably the most commonly used demand model in the empirical literature as of now, and is also very popular in non-research applications of IO. Utility is given by: $$ U_{ij} = f(v_i, p_j) + \sum_{k}\sum_{r} x_{jk}v_{ir}\theta_{kr} + \varepsilon_{ij} $$ where the $\varepsilon_{ij}$ extreme-value Type I error term has been added.

\subsection{Data and issues with estimation}



\chapter{Estimating demand for vertically differentiated products}



\chapter{Logit, Nested Logit and Multinomial Probit}

\section{"Plain-vanilla" Logit}

\subsection{Overview}

Recall that in the pure logit model, each product has a mean quality level $\delta_j$ that incorporates the characteristics of the product as well as its price, and an idiosyncratic error term $\varepsilon_{ij}$ that is drawn iid, from a Type-I Extreme Value (or Gumbel) distribution. The Gumbel distribution has a pdf of $f(x) = e^{-x}e^{-e^{-x}}$ and a cdf of $F(x) = e^{-e^{-x}}$. We assume the variance of that distribution to be equal to $\pi^2/6$ which will normalize the scale of utility as we'll see later. The mean of the distribution is non-zero but we'll see shortly why it is not an issue. Formally, the utility derived from consumption of a good $j$ by consumer $i$ is written as: $$ U_{ij} = \delta_j + \varepsilon_{ij} $$ where $\delta_j = X_j\beta - \alpha p_j + \xi_j$. We say that a consumer chooses a good $j$ over $k$ if and only if his utility from good $j$ is strictly higher than from good $k$.

Therefore, the probability that consumer $i$ buys good $j$ is the probability that he chooses $j$ over any $k$: \begin{align*} P_{ij} = \prob{U_{ij}>U_{ik}\text{ for all } j\neq k} & = \prob{\delta_j + \varepsilon_{ij} >\delta_k + \varepsilon_{ik}\text{ for all } j\neq k} \\
& = \prob{ \varepsilon_{ij} - \varepsilon_{ik} >\delta_k - \delta_j \text{ for all } j\neq k }
\end{align*} 

It turns out that if $\varepsilon_{ij}$ and $\varepsilon_{ik}$ follow a type-I extreme value distribution independently, then the difference between both follows a logistic distribution with cdf $F(x) = \frac{e^{x}}{1+e^{x}}$. This form of the probability also tells us that constant terms in the utility across products do not matter since we consider only $\delta_j - \delta_k$ which exactly cancels any constant that we could observe. Using this fact, we can normalize utility by substracting a same constant to all values, in particular, we can set the utility of an outside good to 0, so that $\delta_0 = 0$.

Since all error terms are independent of each other, we can write the following:\begin{align*} P_{ij}\vert \varepsilon_{ij} & = \prob{ \varepsilon_{ik} < \varepsilon_{ij} + \delta_j - \delta_k \text{ for all } j\neq k } \\ & = \prod_{k\neq j} F(\varepsilon_{ij} + \delta_j - \delta_k)
\end{align*} and since $\varepsilon_{ij}$ is not known, we compute the unconditional probability by integrating over all possible values of $\varepsilon_{ij}$ from the Gumbel distribution: $$ P_{ij} = \int \prod_{k\neq j} F(\varepsilon_{ij} + \delta_j - \delta_k) f(\varepsilon_{ij})\D\varepsilon_{ij} = \frac{ e^{\delta_{j}} }{ \sum_{k=0}^{J} e^{\delta_{k}} } $$

You can see that the probability of consumer $i$ buying good $j$ does not depend on any individual-only parameter or variable, so that we can express this probability of buying a good as the market share of that good $s_j$. Moreover, since we assume that utility for the outside good was 0, we have that $e^{\delta_0} = 1$. This yields the following market shares: $$s_j(\theta) = \frac{\exp(\delta_j)}{1 + \sum_{k} \exp(\delta_{k})} $$ $$s_0(\theta) = \frac{1}{1 + \sum_{k} \exp(\delta_{k})} $$

This model is very practical in the sense it is very easy to set up and displays several desirable properties:\begin{itemize}
\item $s_j = P_{ij}\in(0,1)$ so that any market share can be rationalized by the model.
\item As $\delta_{j}$ increases, reflecting higher attributed quality, the market share will increase to 1. If $\delta_j$ decreases, then the market share goes to 0. However, note that these results only hold at the limit, meaning a good will never have a market share equal to 0 or 1. If that is the case in the data, the researcher should take the product out of the dataset. This is why estimating a logit model must be done on a time-period long enough so that all goods considered have been sold at least once (think about vending machines).
\item $\sum_j s_j = 1$, meaning that one alternative will be chosen, always.
\item $s_j = P_{ij}$ is everywhere continuously differentiable in the characteristics $\delta_{j}$ and in price $p_j$.
\end{itemize}

The last point is useful to compute price derivatives (and elasticities): \begin{align*}
\frac{\partial s_j(\theta)}{\partial p_j} = \frac{-\alpha e^\delta_j \cdot (1 + \sum_{k}e^\delta_k ) + \alpha e^\delta_j e^\delta_j }{(1 + \sum_{k} e^\delta_k )^2} & = \frac{-\alpha e^\delta_j\cdot\left[ 1 + \sum_{k}e^\delta_j - e^\delta_j\right]}{(1 + \sum_{k}e^\delta_j)^2} \\ 
& = - \alpha \cdot \frac{e^\delta_j}{1 + \sum_{k}e^\delta_j} \cdot \frac{1 + \sum_{k}e^\delta_k - e^\delta_j}{1 + \sum_{k}e^\delta_k} \\
& = - \alpha s_j (1 - s_j)
\end{align*} And using the same method, we find $\partial s_j(\theta)/\partial p_k = \alpha s_j s_k $

\subsection{Logit and taste variation}\label{sssec:tastevar}

The logit model is very efficient and easy to set up if you need to measure systematic taste variation, that is, variation in tastes associated with observable characteristics $X_j$. Variations associated with idiosyncratic randomness, $\varepsilon_{ij}$ are assumed away as type-I extreme value random variables.

In reality, the value that agents put on a particular attribute varies across these individuals. Sometimes these tastes vary in an identifiable way, for example, low-income agents might more concerned about the price than others; sometimes you might observe people with the exact same observable characteristics choosing different options. Logit models allow for identification of these taste variations only within limits: if this variation is linked with observable characteristics, then logit models can be applied; if this variation is purely random, then we will require other models.

As an example, consider the choice of households between cars, where two characteristics enter the decision: price $p_j$ and shoulder room $x_j$. A household $i$ places value $U_{ij}$ on buying good $j$ such that: $$U_{ij} = \beta_i x_j - \alpha_i p_j + \varepsilon_{ij} $$ where you can see that the parameters $\alpha, \beta$ vary across households $i$. Say the shoulder room taste $\beta_i$ depends only on the number of members in the household $m_i$ such that $\beta_i = \rho M_i$; this means that $m_i$ is positively correlated with $\beta_i$. Similarly let's say the importance of price is negatively correlated to income $I$ so that $\alpha_i = \theta/I_i$. Substituting back into the utility function we get: $$U_{ij} = \rho M_i x_j - \theta p_j/I_i + \varepsilon_{ij} $$ which can be estimated with a standard logit model where variables are interaction between characteristics of the product and characteristics of the household.

In contrast, suppose taste variation was subject to an error term such that $\beta_i = \rho M_i + \mu_i$ where $\mu_i$ is non-observable (a random variable). Then the utility would be written as: $$U_{ij} = \rho M_i x_j - \theta p_j/I_i + \varepsilon_{ij} + \mu_ix_j $$ and the logit would confound the error term with $\tilde\varepsilon_{ij} = \varepsilon_{ij} + \mu_ix_j$ which is clearly correlated across product specifications and households.

Bottomline is logit models can handle systematic taste variation but not random taste variation. For the latter, we will need more complex models, such as the BLP model studied in the next chapter.

\subsection{Too-many-parameters problem}

The estimation of the quality characteristic $\delta_j$ is done over the set of non-price characteristics $X_j$. In particular, recall that we've seen that:
$$ \delta_j = X_j\beta - \alpha p_j + \xi_j $$ 

In order to identify correctly this quality term, we need that the number of different observed characteristics (the dimension of $X_j$, say $k$) is smaller than the number of products $j$. This also means that we will need to include the term $\xi_j$ in our regression in order to explain any pattern of market shares that would be unobserved by the econometrician.

However, if consumers know $\xi_j$, firms must also know it and price their products accordingly. This introduces endogeneity in the error term with respect to prices. Thus, the econometrician needs instruments to correct for that endogeneity. Typically, these instruments include cost shifters that would not affect intrinsic demand for the good but would definitely have an effect on prices.

Note that any non-price characteristic included in $X_j$ that would also be correlated with the unobservable characteristics $\xi_j$ needs to be instrumented as well.

\subsection{Derivatives and Elasticities}

The logit model presented above displays features that are not desired in the context of elasticities. These problems imply that one would run into issues while trying to expand logit results in terms of welfare analysis, antitrust analysis, etc.

First, recall the own- and cross-price derivatives and elasticities:
$$ \frac{\partial s_j}{\partial p_j}  = -\alpha s_j (1 - s_j) ; \quad \frac{\partial s_j}{\partial p_k}  = \alpha s_j s_k ; \quad \epsilon_{jj}  = -\alpha p_j (1 - s_j) ; \quad \epsilon_{jk}  = \alpha p_k s_k $$

These imply that:
\begin{itemize}

\item Two products with the same market shares will have the same markups. Indeed, from the first-order conditions, we have that: $$p_j - mc_j = \frac{1}{\vert\epsilon_{jj}\vert} \Leftrightarrow \frac{p_j - mc_j}{p_j} = \frac{1}{\alpha(1 - s_j)} $$ which would be the same for $s_j = s_k$. This is obviously not intuitive but also not observed in reality. 

\item Own-price elasticities are higher for higher priced goods. This fact comes directly from the formula of the own-price elasticity that shows a positive effect (in absolute value) from the prices. This is counter-intuitive since it would imply that people buying higher-priced items would be more price-sensitive than people buying lower-priced goods.

\item Substitution between goods only depends on relative shares and not proximity of product characteristics. In fact, cross-price elasticities are given by $\frac{\partial s_j}{\partial p_k}\frac{p_k}{s_j} = \alpha p_k s_k $, which is not a function of characteristics of neither products.

\end{itemize}

\subsection{Independence of Irrelevant Alternatives (IIA)}\label{sssec:logitiia}

We have seen in the overview of the logit model that idiosyncratic errors are independently distributed across products, following a type-I extreme value distribution. The choice of the distribution turns out not to be very important, but the independence property has some implications that the researcher should know about. In fact, the independence assumption means that the unobserved utility derived from one good ($\varepsilon_{ij}$) is unrelated unconditionally to the unobserved utility from another good. This assumption seems to be rather restrictive in the context of goods but helped us a lot in coming up with the solution of the logit model.

The independence of the error terms turns out to have problematic implications in the realism of the choice mechanism. In fact, suppose a consumer has a choice of going to work by car ($c$) or taking a blue bus ($bb$). Say that utility derived from both models are the same, such that $P_c = P_{bb} = 1/2$, meaning the ratio of probabilities is one. Now suppose that a red bus ($rb$) is introduced in the market such that it is identical in every aspect except the color to the blue bus. The probability of taking the red bus should therefore be the same as taking the blue bus: $P_{rb}/P_{bb} = 1$. However, the ratio of probabilities between the car and the blue bus has not changed because of independence of irrelevant alternatives, meaning that $P_{bb}/P_{c} = 1$, thus $P_c = P_{bb} = P_{rb} = 1/3$ is the prediction of the logit model. In real life though, we would expect the probability to take the car to remain exactly the same since the actual problem is to either take the bus (regardless of the color) or the car, yielding $P_c = 1/2, P_{bb}=P_{rb} = 1/4 $.

This IIA problem is nonetheless also a feature of the logit model when it corresponds to reality. In fact, this property allow the researcher to consider only subsets of the complete set of alternatives and still get consistent estimates, as long as for each observation, the actual choice is kept in the set. Another practical use of that property is that if the researcher is only interested in a few choices, then they do not need to include the other choices in the dataset, leaving the data research part out of the picture.

One could test for these properties by, ...

\subsection{Consumer Surplus}

For policy analysis, the researcher is often interested in measuring the change in consumer surplus that is associated with a particular event (introduction of a product, merger, etc.). Under the logit assumption, this value of the consumer surplus also takes a simple closed form that is easily found from the model. We know that each consumer will choose the product that yields the highest utility. In the aggregate case, all consumers have the same tastes so that in expectation (the econometrician does not observe $\varepsilon_{ij}$), consumer surplus is defined as the value of utility derived from the best good. Formally, $$\E{\operatorname{CS}} = \E{\max_{j} \{U_{ij}\}} = \E{\max_{j} \{\delta_{j} + \varepsilon_{ij} \}}$$ which yields: $$\E{\operatorname{CS}} = \ln\left(\sum_{j=1}^{J} e^{\delta_j} \right) + C $$ where $C$ is an unknown that represents the fact that absolute utility cannot be measured. This value is called the logit inclusive value, or the log-sum term. As you can see, comparing two policies is easy in this setting, let one policy be denoted by the superscript $0$ and the other by the superscript $1$. Then, $$\Delta\E{CS} = \ln\left(\sum_{\mathcal{J}^1} e^{\delta_j^1} \right) - \ln\left(\sum_{\mathcal{J}^0} e^{\delta_j^0} \right) $$

If we had transaction-level data with individual characteristics, we could perform this analysis of consumer surplus at each unit of observation and aggregate to find our results.

\section{Nested Logit}

A natural extension of the simple logit model, allowing for richer substitution patterns and a somewhat less restrictive IIA property is the nested logit model.

A nested logit model partitions the choice set in different subsets called nests such that the actual choice of one good follows from choices among nests. Consumers choose a category of products (modelled as a simple logit), then within a category, they choose a product (again modelled as a simple logit). This sequence of logit model creates a different type of model called the nested logit, where the nests are the products categories. These nests are chosen by the researcher (which is not ideal) and they provide a strong structure to the model which could affect results significantly. Following the notation of \cite{cardell91} and \cite{berry94}, we model utility as: $$U_{ij} = \delta_j + \zeta_{ig} + (1 - \sigma)\varepsilon_{ij} $$ where the new terms are: $\zeta_{ig}$ an idiosyncratic "nest" taste shock that applies to all goods $j$ in the nest $g$ ; and $\sigma$ a parameter of correlation in tastes within nests (if $\sigma$ is high, tastes within group are very correlated and the nest structure matters, if $\sigma$ is low, then the correlation in tastes within the nest is small and the nest structure is irrelevant). The $\zeta_{ig}$ error term in this utility follows the unique distribution distribution function such that $\zeta_{ig} + (1 - \sigma)\varepsilon_{ij}$ follows an extreme-value distribution (not type-I however, we call it Generalized Extreme Value or GEV).

We know that within the same nest, the utility level  $u_{ij} = \delta_j + \zeta_{ig} (1 - \sigma)\varepsilon_{ij}$, can be reduced down to ignore the effect of $\zeta$, since it is the same across products. We end up in the same setting as the simple logit model. This implies that the conditional share of product $j$, or the share within the nest, is given by the same formula as the simple logit: $$s_{j\vert g} = \frac{\exp(\delta_j/(1-\sigma))}{\sum_{k\in\mathcal{J}_g} \exp(\delta_k/(1-\sigma))} $$ where the denominator for this expression can be written as $D_g$, the total demand for products in the group $g$.

Meanwhile, across groups, we have that both error terms still give a type-I EV, so again, we can think about the demand for a group as we did in the logit case: $$s_g = \frac{D_g^{(1-\sigma)}}{\sum_{h} D_h^{(1-\sigma)}} $$ Finally, the share of a product $j$ is given by the product of both the share of the group containing $j$ and the conditional share of $j$ within the group: $$s_j = s_{j\vert g} \cdot s_g = \frac{\exp(\delta_j/(1-\sigma))}{D_g^\sigma\cdot\left(\sum_{h} D_h^{(1-\sigma)}\right)} $$ As we can see, demand for product $j$ depends on its own quality level relative to its group, the quality of the group relative to the other groups and $\sigma$, the correlation in tastes within nests.

The outside good in this model is considered as one of its own group, so that $s_{0\vert 0} = 1$ and with the normalization of $\delta_0 = 0$ and $D_0 = 1$, we get:$$s_{0} = 1\cdot s_0 = \frac{1}{\sum_h D_h^{(1-\sigma)}} $$

This analytical derivation is the same when we extend the model to have nests within nests and more.

\subsection{IIA and substitution patterns}

The nest structure of this model satisfies two properties:\begin{enumerate}

\item For any two alternatives that are in the same nest, the ratio of probabilities is independent of the attributes and/or existence of other products in the nest or in other nests. $$ s_j / s_k = s_{j\vert g} / s_{k\vert g} = \frac{\exp(\delta_j/(1-\sigma))}{\exp(\delta_k/(1-\sigma))} $$ Equivalently, we say that IIA holds within the nest.

\item For any two alternatives that are in different nests, the ratio of probabilities depends on the attributes and/or existence of other products in the two nests. $$ s_j / s_k = s_{j\vert g} / s_{k\vert h} \cdot s_g / s_h  = \frac{\exp(\delta_j/(1-\sigma))}{\exp(\delta_k/(1-\sigma))} \cdot \frac{D_g^{1-\sigma}}{D_h^{1-\sigma}} $$ Equivalently, we say that IIA does not hold across nests.
\end{enumerate}

\subsection{Research considerations}

As we have seen, the nested logit is a fairly simple extension on the simple logit case, which relies on relaxation of the IIA property across groups (or nests). The model is often told in a narrative of sequential choice: consumers first choose a group $g$ ($s_g$), then a product (or group) $j$ within group $g$ ($s_{j\vert g}$). 

The results of the model depend very strongly on the ex ante nesting structure chosen by the researcher. Therefore, it is very important (and hard) to understand what the appropriate structure should be. The sequential narrative is supposed to help the researcher come up with an accurate structure but it might be unhelpful at times.

Identification comes in different flavors:\begin{itemize}
\item Parameters associated with characteristics and prices are identified within group by variation in these exact characteristics.
\item The correlation in tastes parameter ($\sigma$) is identified by variation in 
\end{itemize}

\subsection{Extensions of the GEV}



\section{Multinomial Probit}

The multinomial probit is a more flexible alternative to the simple logit model. In fact, by allowing errors $\varepsilon_{ij}$ to be correlated (not iid) and to follow a multivariate normal distribution $N(\mu, \Sigma)$ that is completely free, it achieves more interesting outcomes than the simple logit. For example, with an unrestricted variance matrix $\Sigma$, you could end up with substitution patterns that are more...

\section{GMM Estimation Strategy for Product-level data}

The general estimation strategy that applies to logit models with product-level data is detailed in the following steps:\begin{enumerate}
\item Assume that data is drawn from markets with large $n$.
\item Assume that observed market shares are measured without errors.
\item For each $\theta$ (vector of parameters), there exists a unique $\xi$ such that the model shares and observed shares are equal ($J$ equations, $J$ unknowns).
\item We invert the model to get $\xi$ as a function of the parameters. How this step is performed depends on the functional form of the model.
\item Using $\xi$, we can create the moments of the model, estimating them by GMM. 
\end{enumerate}

\subsection{Inversion in the logit case}

We need to express the quality unobservable term $\xi_j$ in terms of all observable characteristics. First, recall that: $$ \delta_j = X_j\beta - \alpha p_j + \xi_j $$ but in this equation, $\delta_j$ is unknown (there is no such thing as a perceived mean quality level). Nevertheless, we can use the market shares formulas that link the $\delta_j$ to the observed market shares $s_j$ (observed without errors). Thus, $$s_j = \frac{\exp(\delta_j)}{1 + \sum_k \exp(\delta_k) } $$ but again, we run into a problem since it is expressed as a function of other $\delta_k$: we could solve a system of equations, or simplify the model using the normalized good. Indeed, $$\frac{s_j}{s_0} = \exp(\delta_j) \Leftrightarrow \ln(s_j) - \ln(s_0) = \delta_j \Leftrightarrow \ln(s_j) - \ln(s_0) = X_j\beta - \alpha p_j + \xi_j $$ which in turn yields the inversion equation: $$
\ln(s_j) - \ln(s_0) +  \alpha p_j  - X_j\beta = \xi_j $$

\subsection{Inversion in the nested-logit case}

In the same way as in the simple logit model, our main objective is to get the link between $\delta_j$ and observables so that we can identify $\xi_j$. In this case: $$s_j = \frac{\exp(\delta_j/(1-\sigma))}{D_g^\sigma\cdot\left(\sum_{h} D_h^{(1-\sigma)}\right)} \text{ and } s_0 = \frac{1}{\sum_{h} D_h^{(1-\sigma)}} $$ so that $$\ln(s_j) - \ln(s_0) = \frac{1}{1-\sigma}\cdot \delta_j - \sigma\ln(D_g) $$ We turn into a new problem caused by the two-level structure, which is $D_g$ is not parameterized. Again, we solve this issue by using the normalized good: $$ s_g / s_0 = D_g^{(1-\sigma)} \Leftrightarrow \ln(s_g) - \ln(s_0) = (1-\sigma) \cdot \ln(D_g) $$ which we can plug back in the previous equation to get: $$\ln(s_j) - \ln(s_0) = \frac{1}{1-\sigma}\cdot \delta_j - \frac{\sigma}{1 - \sigma} [ \ln(s_g) - \ln(s_0) ] $$ $$\Leftrightarrow (1 - \sigma) \cdot [\ln(s_j) - \ln(s_0)] = \delta_j - \sigma [ \ln(s_g) - \ln(s_0) ] $$ $$\Leftrightarrow (1 - \sigma) \cdot \ln(s_j) + \sigma \ln(s_g) - \ln(s_0) = \delta_j $$ Finally, using the fact that $s_g = s_j / s_{j\vert g} \Leftrightarrow \ln(s_g) = \ln(s_j) - \ln(s_{j\vert g})$, we can get: $$\Leftrightarrow \ln(s_j) - \sigma \ln(s_{j\vert g}) - \ln(s_0) - X_j\beta + \alpha p_j = \xi_j $$

\subsection{Endogeneity issues}

Regardless of the model we use, we will have to deal with endogeneity issues regarding many dimensions, such as prices, observable characteristics or market shares. 

In fact, in both models price was correlated to the unobservable term as firms make their decision based on demand which includes this term; moreover, in the nested logit case, we also have to deal with endogeneity in the within-share term. In general, anything that is correlated with the unobservable quality term $\xi$ will have to be instrumented.



In the particular case of the nested logit model, you should realize that conditional market shares and actual market shares depend on the same "function", which means we also need to instrument for one of these shares. ...

\subsection{Measurement errors}

Measurement errors in observed prices, characteristics or quantities may also create difficulties for the estimation procedure outlined above. Prices enter linearly in the estimation and are already endogenous so that these errors do not create too important biases. However, when these errors are present in market shares or quantities data then this issue becomes more important. Indeed, market share data is used to invert the model and get the unobservable term as a function of observed data. This non-linear inversion will in fact be very sensitive to measurement errors in the market shares.

\subsection{Adding supply-side restrictions}



\section{Individual-level data}



\chapter{Mixed Logit (BLP)}

For now we have seen three types of discrete-choice models and their applications to demand estimation for differentiated products: the simple logit model, the nested logit model and the multinomial probit model. These three models, although quite useful and fairly simple to estimate were limited in three dimensions: they do not allow for random taste variation (\ref{sssec:tastevar}), they have restricted substitution patterns (\ref{sssec:logitiia}),  they do not allow for correlation over time. Mixed logit models (which contains BLP) are highly flexible models that can deal with the previously mentioned issues.

Mixed logit models are a class of models that encompasses all types of models where the market shares are computed as integrals over simple logit functional form. We'll see in detail later what that means but intuitively, you should think of the model as everyone having her/his own logit model of demand, and aggregate demand would be computed by integrating over consumer attributes. In particular, IO economists are most interested in the BLP (for \cite{blp_95}) model and extensions of it like \cite{dfs_12}.

We'll first cover the basics of mixed logit and random coefficients, before talking in depth about estimation techniques as found in BLP and DFS.

\section{Model}

Generally, as stated in \cite{blp_95}, we write utility derived from consumption of a good $j$ by consumer $i$ as the function $u_{ij}(X_j, p_j, \xi_j, \nu_i, \theta)$, which is a function of product $j$ observable characteristics ($X_j$), price ($p_j$), unobservable characteristics ($\xi_j$) and consumer characteristics (observable $z_i$ and unobservables $\nu_i$), all entering the utility function through a vector of parameters $\theta$. 

As a simpler case, define utility as a linear function of those parameters such that: $$U_{ij} = X_j\beta_i + \xi_j + \varepsilon_{ij} $$ Notice the main differences with logit models as we know them: first, price is not separated but included in observable characteristics of product $j$ because of the second point, that $\beta_i$ is a coefficient dependent on consumer characteristics. This means that different consumers will have different tastes in the same characteristics. For example, some person might be interested in the design of a phone while someone would disregard this and focus only on memory, another one on camera quality, etc.

\subsection{Latent-class models}

As a simple case, consider that there are only two types of people, such that only two $\beta_i$ are possible, say $\beta_H$ and $\beta_L$.



\subsection{Random coefficients models}

Further, we will assume that this "taste variation component" $\beta_i$ has a nice linear structure such that: $$\beta_i = \lambda + z_i\beta^o + \nu_i\beta^u $$ thus yielding the following utility: $$ U_{ij} = X_j\lambda + X_jz_i\beta^o + X_j\nu_i\beta^u + \xi_j + \varepsilon_{ij} $$ $$ = \delta_j + X_jz_i\beta^o + X_j\nu_i\beta^u + \varepsilon_{ij} $$ where $\delta_j$ is the mean utility level as in previous models. In this sense, the linear model we have just described can model logit models if we assume these interaction terms to be 0. 

These interactions are of two types (observables and unobservables) and together they remove the IIA problem when aggregating over consumers, since they include utility based on characteristics of people buying the products: this creates a "closeness" between products that was not observed in simple logit models. Nevertheless, at the individual level, the IIA problem remains since for a single consumer, this interaction term disappears and you are left with a simple logit model.

\subsubsection{Substitution, Markups and Strategic complementarity}



\subsubsection{What type of data should you get?}



\section{BLP algorithm}

Consider the situation where data is available only on the aggregate product-level, and no consumer data is observable. We will workout the estimation of demand following four steps:\begin{enumerate}
\item Work out the aggregate shares conditional on both $\delta$ (mean utility) and $\beta$ (taste variation).
\item Invert the shares to get $\xi$.
\item Estimate the model using the method of moments.
\item Repeat until convergence of all parameters.
\end{enumerate}

To help understanding the details of the following steps, you should keep in mind a quick summary of what it to be done. As always, we are interested in the parameters of utility (that affect demand), thus $\lambda$ and $\beta^u$ (that we we now call $\beta$ only). These parameters are estimated using the interaction of the unobservable product characteristic $\xi$ and adequate instruments. Until now, nothing should surprise you as we follow the same steps as described the GMM estimation strategy for logit and nested-logit models. However, this time it is different since to get $\xi$, you will need the parameters you are looking for (this is the main difference of BLP). That is why you use starting values for those, and iterate until you find the parameters that are stable through estimation (this is a fixed point problem).

\subsubsection{Step 1: Conditional aggregate shares}

Recall that in the simple and nested logit models we studied, the probability $P_{ij}$ that a consumer $i$ buys product $j$ was equal to the market share since they did not differ from the representative consumer in any way. This time, we now that two different consumers will have different probabilities of buying the product. Consequently, the market share is going to be the integral of consumers individual probabilities over their characteristics (here $\nu_i$ since we do not observe any consumer characteristics).

Our utility function looks like: $$U_{ij} = \delta_j + X_j\nu_i\beta^u + \varepsilon_{ij} $$ thus the individual probability of buying product $j$ is given by $$P_{ij} = \frac{\exp(\delta_j + X_j\nu_i\beta^u)}{1 + \sum_k \exp(\delta_k + X_k\nu_i\beta^u) } $$ and finally, integrating this over the space of $\nu$, we get the market shares as a function of product characteristics: $$s_j(\delta, \beta) = \int \frac{\exp(\delta_j + X_j\nu_i\beta^u)}{1 + \sum_k \exp(\delta_k + X_k\nu_i\beta^u) } f(\nu) \D\nu $$
The issue with this integral is that it cannot be solved analytically ($\nu_i$ is usually a multivariate normal distribution, which makes the market share a multi-dimensional integral); we can approximate it by taking the sample average over a set of $ns$ draws in a simulation. This yields: $$\hat s_j^{ns}(\delta, \beta) = \frac{1}{ns} \sum_{i} \frac{\exp(\delta_j + X_j\nu_i\beta^u)}{1 + \sum_k \exp(\delta_k + X_k\nu_i\beta^u) } $$ which is a function of parameters that we want to estimate ($\delta$ and $\beta$). We will see that $\delta$ will be computed, and $\beta$ will be estimated by doing this step multiple times and finding the best one. This is why you should \textbf{KEEP YOUR FIRST SIMULATION OVER} $\nu$!! You do not want to change it, else it is never going to converge.

Note that integration over the distribution of $\nu_i$ is a different problem than $\varepsilon_{ij}$ for which we can use the form of the univariate Gumbel distribution to help us with analysis. In the case of $\nu_i$, we assume a multivariate normal distribution (across multiple consumer unobserved characteristics). If the distribution (not observations, but at least the pdf) is observed, then we can draw from the said distribution.

Moreover, notice that the use of a finite number of draws in the simulation will create a new source of errors within our estimation routine. Enough simulation draws should help tampering this issue, although finding the good number of draws is not an exact science, but more like a tradeoff between computation speed and errors. Overall, numerical evaluation of integrals is a particular topic that is deep enough to think about it carefully.

\subsubsection{Step 2: BLP inversion}

We now need to recover the product unobservable term $\xi_j$. In the same way as we did in the simple logit models, we already have the link between $\delta$ and $\xi$, but $\delta$ is "buried" in a nonlinear fashion into the the market shares: we need to invert the market shares to get delta, thus $\xi$, as a function of the shares (rather than the opposite). Doing that requires a special trick that is at the very core of\cite{blp_95} contribution.

Their trick is to use the fact that the following system: $$\delta_j^k(\beta) = \delta_j^{k-1}(\beta) + \ln(s_j) - \ln(\hat s_j^{ns}(\delta^{k-1}, \beta)) $$ is a contraction mapping. To see it, understand that $s_j$ is the observed market share, the exponent $k$ represents the iteration process. In other words, by iterating over this function, the $\delta$ values will converge to the true value, $\delta^*(\beta, s, \hat s)$, where $s, \hat s$ are respectively the vectors of observed and estimated shares conditional on $\beta$. Finally, we can write: $$\xi(\beta, s, \hat s) = \delta^*(\beta, s, \hat s) - X_j\lambda $$ and use this form to construct the moments.

\subsubsection{Step 3: Constructing the moments}



\subsubsection{Step 4: Algorithm iteration}

The first three steps were performed for a given $\beta$ 

\section{DFS algorithm}



\chapter{Product Availability}

\section{Introduction}

Previous chapters have introduced demand estimation under 

\chapter{Entry Models}

\section{Introduction}



\section{Static and exogenous models}

The main question of the book from \cite{sutton_91} revolves around how do markets evolve to be less or more concentrated? Moreover, Sutton tries to explain why advertising is higher in concentrated industries. In order to explore the answers to these questions, he looks at different market structures and analyzes what makes these structure coherent with the data.

\subsection{Perfect competition}

Consider a market with perfect competition (price-taking assumption), exogenous sunk costs and free entry. The minimum efficient level of production (in the long run) is where $p = \min AC $. 

This assumption means that, in the long run, firms will produce only the quantity satisfying this assumption, not more, not less. This implies that the number of firms in a market only depends on the size of the market, $M$. In particular, if you denote the quantity produced by firms at the optimum as $q^*$, you will have that the optimal number of firms $n^*$ is given simply by: $$n^* = M/q^* $$
As $M\to\infty$, we also have $n^*\to\infty$, thus the concentration ratio will tend to 0.

The issue with this simplistic model is that it might not hold in many settings where (1) competition might be imperfect or (2) sunk costs might be endogenous. As an example consider the increase of population in the United States since the creation of Pepsi and Coca-Cola. Even though the population has more than doubled, their respective market shares are still about 30\% and 60\%, which clearly disproves the previous model. For that reason he explores both directions.

\subsection{Imperfect competition}

Now suppose you allow for imperfect competition. For that purpose, we consider a two-stage game where in the first stage, firms choose to enter a market and incur a cost of $A$; then in the second stage, firms play the market game.

Moreover, assume that all $M$ consumers have an income of 1 to spend exclusively on the good produced by the firms, such that in equilibrium: $$p\cdot Q = M$$
Finally, suppose the marginal cost of production is $c>0$.

\subsubsection{Bertrand competition}



\subsubsection{Cournot competition}

In the Cournot solution, assuming $N$ firms enter, a firm $i$ will choose its quantity $q_i$ such that: $$q_i \in \arg\max_{q_i} \left[M\cdot\left(\sum_{j=1}^{N} q_j\right)^{-1} - c\right]\cdot q_i $$ which yields the following FOCs: $$\left[M\cdot\left(\sum_{j=1}^{N} q_j\right)^{-1} - c\right] - q_i\cdot M \cdot \left(\sum_{j=1}^{N} q_j\right)^{-2} = 0 \text{ for all } i = 1, 2, ..., N $$
$$\Leftrightarrow \left[M\cdot Q^{-1} - c\right] - q_i\cdot M \cdot Q^{-2} = 0 \text{ for all } i = 1, 2, ..., N $$ If we add all $N$ conditions together, we get: $$ N\cdot\left[M\cdot Q^{-1} - c\right] - Q \cdot M \cdot Q^{-2} = 0 $$ $$\Leftrightarrow N \cdot M\cdot Q^{-1} - N \cdot c - M \cdot Q^{-1} = 0 $$ $$\Leftrightarrow \frac{N - 1}{N} \cdot M\cdot Q^{-1} = c $$
Finally recall that $M\cdot Q^{-1}$ is actually the equilibrium price, meaning that we have: $p = \frac{N}{N-1} c = (1 + \frac{1}{N-1})c $. Consequently, we have: $$Q = \frac{N - 1}{N} \cdot \frac{M}{c}; q_i = \frac{N - 1}{N^2} \cdot \frac{M}{c}; \Pi(N) = \frac{c}{N-1}\cdot \frac{N - 1}{N^2} \cdot \frac{M}{c} = \frac{M}{N^2} $$



\subsubsection{Hotelling/Salop competition}



\section{Static and endogenous models}

\subsection{Sutton's view}



\subsection{Bresnahan and Reiss (1991)}

In a realistic research setting, it is hard to observe strategic variables on a market level (prices, costs, advertising expenses, for all firms). This is the reason why \cite{br_91} innovated on an estimation using mainly superficial market observables: the number of firms (but not the shares) and other market characteristics. The main assumptions of this model rely on a static Sutton model with symmetric firms and free entry.

\subsubsection{Behavioral model}

As in the Sutton-type of models, we solve it backwards, assuming the fixed costs of entry are incurred as sunk in the second period, when firms are choosing production.

Demand in the market $m$ is given by: $$ Q_m = d(Z_m, p)\cdot S(Y_m) $$ where $d(\cdot)$ represents the demand function of a representative consumer and $S(\cdot)$ is the total number of consumers that would buy the product. Note that this demand specification has constant returns to scale: doubling $S$ will double $Q$. Finally, we define the inverse demand curve as $P(Q, Z, Y)$.

Therefore in the second-stage under Cournot competition, each firm solves: $$\max_{q_i}\Pi_{N,m} \equiv P(q_i, q_{-i}, Z_m, Y_m) \cdot q_i - F_N - C(q_i) $$ where $F_N$ is the endogenous sunk cost associated with $N$ firms entering the market. Without loss of generality (we proved a similar result in the Cournot-Sutton context), assume that in equilibrium, quantities will be symmetric such that $q_i = q_j = q^*$ for all $i,j$. Because $N$ is already "chosen" in the second stage, we can write $q^* \equiv d(Z_m, p)\cdot \frac{S(Y_m)}{N}$. Then, using this, the profits of each firm is given by: $$\Pi_{N,m} = P(q^*, q^*, Z_m, Y_m) \cdot q^* - F_N - C(q^*) = \left(P_N - \frac{C(q^*)}{q^*} \right) \cdot d(Z_m, P_N)\cdot \frac{S}{N} - F_N $$ Now, for both the average variable cost $\frac{C(q^*)}{q^*}$ and the fixed cost $F_N$, let's allow them to be additively separable in components, one of them being dependent on the firm only (respectively $AVC$ and $F$) and the other component being endogenous on the number of firms (respectively $b_N$ and $B_N$), such that the total profit of an individual firm, facing a market with $N$ total firms, is defined as: $$\Pi_{N,m} = \left(P_N - AVC(q^*) - b_N \right) \cdot d(Z_m, P_N)\cdot \frac{S}{N} - F - B_N $$

In the first stage, using the free entry assumption, we know that if $N^*$ firms enter a market $m$, it must be that $\Pi_{N^*,m} > 0$ and $\Pi_{N^*+1,m} < 0$. Conversely, we can look at the entry threshold $s_N$, which is the minimum additional demand, not already covered by existing $N-1$ firms that is required in order for the $N$th firm to enter. This threshold is the value of $s_N \equiv S_N/N$ such that: $$\Pi_{N,m} = 0 \Leftrightarrow \frac{S_N}{N} = \frac{F + B_N}{(P_N - AVC_N - b_N)d_N} $$ We can finally also look at the ratio of successive thresholds: $$s_{N+1}/s_N = \frac{F + B_{N+1}}{F + B_N}\frac{(P_N - AVC_{N} - b_N)d_N}{(P_{N+1} - AVC_{N+1} - b_{N+1})d_{N+1}} $$ Thus, in a fully competitive market, a firm would enter each time its cost to enter could be recovered in the market, making $s_{N+1}/s_N$ tend to 1, as $N\to\infty$.

\subsubsection{Empirical strategy}

The threshold equations derived above ask for a lot of observed variables, namely prices, costs (variable and sunk) and more. In reality, it is very difficult to observe all these at the same time for all firms in a market, thus we need a model that could allow for less information, which is the essence of \cite{br_91}.

Consider a reduced-form profit function given by: $$\Pi_{N,m} = \underbrace{\underbrace{S(Y_m, \lambda)}_{\text{size}} \cdot \underbrace{V_{N}(Z_{m}, W_{m}, \alpha, \beta)}_{\text{variable profit}} - \underbrace{F_N(W_{m}, \gamma)}_{\text{fixed}}}_{\bar\Pi_{N,m}} + \epsilon_m $$ which has an endogenous component $\bar\Pi_{N,m}$, and an error-term $\epsilon_m$ that is not dependent on $N$. This should raise some memories to anyone who covered the demand estimation part of the course. In fact, assuming a specification on the error term reveals the model as an ordered probit model, very similar to the logit or multinomial probit models. To see this, consider a market with $N$ firms, this means that: $$ \bar\Pi_{N,m} + \epsilon_m > 0 \text{ and } \bar\Pi_{N+1,m} + \epsilon_m < 0 $$ which is equivalent to: $$ \bar\Pi_{N,m} > \epsilon_m > \bar\Pi_{N+1,m} $$

Assuming $\epsilon_m$ is drawn from an iid normal distribution with mean 0 and variance $\sigma^2$, we have that: $$\prob{N_m} = \begin{cases} \Phi(\bar\Pi_{N,m}) - \Phi(\bar\Pi_{N+1,m}) & \text{ if }N_m > 1 \\
1 - \Phi(\bar\Pi_{2,m}) & \text{ if }N_m = 1
\end{cases} $$

Before going on to the estimation of the model, we need to look at the reduced-form of each component of the profit function:\begin{itemize}
\item The market size includes a combination of market size characteristics $Y_m$, such as the population, the neighbouring population, growth, commuting possibilities, etc.
\item The variable profit per capita is defined as: $V_N = \alpha_1 + X\beta - \sum_{n=2}^{N} \alpha_n $ where:\begin{itemize}
\item $X$ is a vector of relevant economic variables such as demand characteristics for the product ($Z$) and cost-shifters ($W$).
\item $\alpha_n\geq 0$ is an intercept component such that each firm entering a market has a negative effect on profits. 
\end{itemize}
\item The endogenous fixed costs defined as: $F_N = \gamma_1 + \gamma_Lw_L + \sum_{n=2}^{N}\gamma_n\cdot\gamma_n $ 
\end{itemize}

Finally, the estimation is performed using maximum likelihood. In fact, for each observation of the market, we use the probability function defined above. This gives us a likelihood function as a function of all observed variables described in the list above. Taking the log of it yields the log-likelihood to be maximized in order to estimate the parameters of interest.

\subsection{Berry (1992)}

The previous paper was lacking heterogeneity in the sense that its results applies for the case where all firms have the same costs, same continuation payoffs, etc. A step in the direction of allowing some differentiation was made by \cite{berry_92}, where fixed costs can vary across firms. Thus, the behavioral model stays identical, but the profit of firm $k$ can now be divided into a common component $\nu_m(N)$, incurred by all firms in the same way, and an idiosyncratic component $\phi_{m,k}$ applying only to firm $k$. In particular, we have: $$ \Pi_{N,m,k} = \underbrace{X_m\beta - \delta\ln(N) + \rho u_{m,0}}_{\nu_m(N)} + \underbrace{Z_k\alpha + \sqrt{1 - \rho^2}\cdot u_{m,k}}_{\phi_{m,k}} $$ 

Note that in this equation, the combination of the terms $\rho \cdot u_{m,0} + \sqrt{1 - \rho^2}\cdot u_{m,k} $ actually represents the error term, denoted $\epsilon_{m,k}$. In this setting, $\rho$ represents the correlation between error terms across firms in a given market. Finally, \cite{berry_92} assumes that: $$\epsilon_{m,k} \sim N(0, \Sigma) $$ where $\Sigma$ is a matrix with all off-diagonal terms equal to $\rho$.

As in the demand estimation case, this error term is known to the players (the firms) but not to the econometrician. We are still in the static, full information game where all firms know everything about all other firms. However this time we are not in an ordered probit setting since the very structure of the problem is endogenous to the number of firms. To see that, recall the error of the previous model did not include any other subscript than $m$, while this one includes something about $k$ which is intrinsically linked to the number of firms.

Contrary to the previous models, this one can display multiplicity of equilibria. In fact, although the equilibrium number of firms is unique, the exact firms that enter the market can be different. This implies that we lose information on which firms enter a market. For example, the model might predict that in a given market, two firms will ultimately enter, but you could have firms 1 and 2, firms 2 and 3 or firms 1 and 3. In order to control that issue, the author assumes that firms enter in the order of their profitability. Using that assumption, we can simplify the paper and compute the probability of observing $N$ firms in the market as: \begin{align*}
\prob{n_m = N\vert Z_m} & = \prob{\epsilon_{m,1}, ..., \epsilon_{m,K_m} : \sum_{k=1}^{K_m}\mathbb{I}\{\nu_m(N, Z_m) >\phi_{m,k}\} = N } \\
& = \underbrace{\int\hdots\int}_{K_m\text{ times}}\mathbb{I}\left\lbrace\sum_{k=1}^{K_m}\mathbb{I}\{\nu_m(N, Z_m) >\phi_{m,k}\} = N\right\rbrace\D F(\epsilon_{m,1}, ..., \epsilon_{m,K_m}, \theta)
\end{align*} which is a multi-dimensional integral (of $K_m$ dimensions) and as we've seen in the previous two chapters, it is hard to compute. In order to circumvent this computational issue, we could use the same method as in BLP: an inner loop simulating the sample moment of the integral for each value of $\theta$, and an outer loop solving for the value of $\theta$ that minimizes the square distance between the observed and simulated number of firms. This technique is called the Simulated Nonlinear Least Squares (or SNLS).

Formally, the outer loop finds $$\hat\theta = \arg\min_{\theta} \frac{1}{M} \sum_{m} \left( N_m - \E{n_m\vert Z_m, \theta, \epsilon_{m,1}, ..., \epsilon_{m,K_m}} \right)^2 $$ where the expectation term is approximated by its simulated sample average over random iid draws of $(\epsilon_{m,1}, ..., \epsilon_{m,K_m})$. The simulated sample average is defined as: $$\E{n_m\vert Z_m, \theta, \epsilon_{m,1}, ..., \epsilon_{m,K_m}} \approx \frac{1}{S} \sum_{s} n_{m}^{*, s} (\epsilon^s, \theta) $$ $$\text{ where } n_{m}^{*, s} (\epsilon^s, \theta) \equiv \max_{0\leq n \leq K_m} \left\lbrace n : \sum_{k=1}^{K_m}\mathbb{I}\{\nu_m(n, Z_m) > \phi_{m,n}\vert \epsilon^s \}\geq n \right\rbrace $$ or in words, the maximum number of firms $n$, such that the number of firms entering the market with a positive profit is greater than $n$.

To recap the full estimation algorithm, consider these steps:



\section{Two-period models}

\subsection{Stackelberg's model}

Two-period models have been introduced for the first time in entry models, but they have been used in several other settings since, for example in models of investment, contracting, etc. Typically, these models rely on:\begin{itemize}
\item A first period where the agents choose a state variable that will determine the nature of the game in the second period.
\item A second period where the game is played following the state of the world decided on the first.
\end{itemize}
The solution concept used to determine the outcome of the game is called "subgame perfection" which means a Nash equilibrium is chosen at each step played in the game. We also call this equilibrium the Subgame Perfect Nash Equilibrium, or SPNE. In the simple case of a two-period game, the SPNE is quite easy to determine by backwards induction:\begin{itemize}
\item You solve for the NE of the second period game for each potential game played following first period choices.
\item Assuming the outcomes computed above are realized, you solve for the first period choice that is a NE.
\end{itemize}

\subsubsection{Two-period capacity game}

Consider a typical two-period à la Stackelberg game where a firm chooses a level of capital ($K_1 \geq 0$) in the first period, and the second firm chooses its own level ($K_2 \geq 0$) in the second period. A firm that chooses a capital level of $0$ is choosing to stay out of the market. A firm that chooses a level strictly greater than 0 enters the market, although firm 2 would have to pay a fixed cost of entry equal to $F$.

The profits of firm 1 are: $$\pi_1(K_1, K_2) = K_1\cdot (1 - K_1 - K_2) $$
while the profits of firm 2 are: $$ \pi_2(K_1, K_2) = \begin{cases}
K_2\cdot (1 - K_1 - K_2) - F & \text{ if } K_2 > 0 \\
0 & \text{ if } K_2 = 0
\end{cases} $$

In order to solve the game, we proceed by backwards induction. In the second period, we consider the decision of firm 2 over the level $K_2$. It will choose a level $K_2 > 0$ if and only if the profit associated is greater than 0, meaning that: $$ K_2 (1 - K_1 - K_2) - F \geq 0 \Leftrightarrow K_2 (1 - K_1 - K_2) \geq F $$ In that case, firm 2 will choose: $$K_2^* = \arg\max_{K_2} K_2 (1 - K_1 - K_2) - F $$ which is the solution to the following FOC: $$1 - K_1 - 2K_2^* = 0 \Leftrightarrow K_2^* = \frac{1 - K_1}{2} $$ Plugging this solution into the participation condition above, we get that: $$  \frac{1 - K_1}{2} (1 - K_1 - \frac{1 - K_1}{2}) \geq F \Leftrightarrow \frac{(1 - K_1)^2}{4} \geq F \Leftrightarrow K_1 \leq 1 - 2\sqrt{F} $$
and thus the optimal level $K_2^*$ is defined as: $$ K_2^* = \begin{cases}
\frac{1 - K_1}{2} & \text{ if } K_1 \leq 1 - 2\sqrt{F} \\
0 & \text{ if } K_1 > 1 - 2\sqrt{F}
\end{cases} $$

Now that the second period is solved, we can go into the first period. Firm 1 will choose a level $K_1$ to maximize its profits, which are now known to be: $$\pi_1(K_1) = \begin{cases}
K_1 \cdot \left(1 - K_1 - \frac{1 - K_1}{2}\right) & \text{ if } K_1 \leq 1 - 2\sqrt{F} \\
K_1\cdot (1 - K_1) & \text{ if } K_1 > 1 - 2\sqrt{F}
\end{cases} $$ From this perspective, it is clear that, for a given $K_1$, allowing the other player to enter the market is worse than deterring entry. However, deterring entry can only be done for $K_1 > 1 - 2\sqrt{F}$. This creates a discontinuous profit function where deterring entry is always better but can be achieved only at some points that might yield a lower profit than allowing firm 2 to enter. The following graph represents this discrepancy nicely:



\subsection{Empirical work}

 

\end{document}