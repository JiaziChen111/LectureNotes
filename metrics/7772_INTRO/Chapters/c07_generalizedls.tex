\section{Heteroskedasticity}

Heteroskedasticity is the phenomenon when error terms $e_i$ do not have the same variance for all $i$. Formally, we write $\E{e_ie_i'} = \sigma^2\Omega$ where $\Omega$ is a diagonal matrix different from the identity matrix and by normalization $\operatorname{tr}(\Omega) = n$.

In this particular case, our typical model for $Y = X\beta +e$ does not satisfy all Gauss-Markov assumptions. The next question we should ask ourselves is how does this violation affects our OLS estimates. Note that we still have: $\hat\beta_{OLS} = \beta + (X'X)^{-1}X'e$. Hence, $$\E{\hat\beta_{OLS}} = \beta + (X'X)^{-1}X'\E{e} = \beta $$ Our OLS estimator is still unbiased, is it consistent?\begin{align*}
\lim_{n\to\infty} \V{\hat{\beta}_{OLS}} = \lim_{n\to\infty}\E{(\hat\beta - \beta)(\hat\beta - \beta)'} & = \lim_{n\to\infty}\E{((X'X)^{-1}X'e)((X'X)^{-1}X'e)'} \\ 
& = \lim_{n\to\infty}\E{(X'X)^{-1}X'ee'X(X'X)^{-1}} \\ & = \lim_{n\to\infty}(X'X)^{-1}X'\E{ee'}X(X'X)^{-1} \\ 
& = \lim_{n\to\infty}\frac{1}{n^2}\left(\frac{X'X}{n}\right)^{-1}X'\sigma^2\Omega X\left( \frac{X'X}{n}\right)^{-1} \\ 
& = \lim_{n\to\infty}\frac{\sigma^2}{n}\left(\frac{X'X}{n}\right)^{-1}\frac{X'\Omega X}{n}\left(\frac{X'X}{n}\right)^{-1} \\ 
& = \lim_{n\to\infty}\frac{\sigma^2}{n}Q_n^{-1}R_n Q_n^{-1}
\end{align*} It turns out that the consistency of $\hat\beta$ depends heavily on the limiting behavior of the term $R_n$. Indeed, since $Q_n$ tends to $Q_0$, a constant, when $n$ grows. We only need that $R_n$ grows at a rate lower than $\sigma^2/n$ to have a variance that tends to $0$ as $n$ tends to infinity.

\subsection{Generalized Least-Squares}

The last result we derived about consistency of the OLS estimator is not satisfying enough, thus we might want to design a better estimator. The intuition needed to build a better one relies on two elements: first, we want an estimator that takes into account the new form of the variance matrix; second, we could transform the variance matrix into an identity matrix and somehow make our OLS estimator work. The Generalized Least-Squares estimator does exactly those two things.

Let $P$ be a matrix such that: $$\V{Pe} = \sigma^2I_n $$ This implies $$\E{(Pe)(Pe)'} = \sigma^2I_n \Leftrightarrow \E{Pee'P'} = \sigma^2 I_n \Leftrightarrow \sigma^2P\Omega P' = \sigma^2 I_n \Leftrightarrow P\Omega P' = I_n$$ This is what we call the spectral decomposition of $\Omega$. Now we have that the term $Pe$ is indeed homoskedastic so we might look for the effects of multiplying our whole model by $P$.

Let $Y^* = X^*\beta + e^*$ where starred variables are the true variables projected by matrix $P$ (i.e. $Y^* = PY$). This model satisfies all Gauss-Markov assumptions when the true model only violates heteroskedasticity. Therefore, we'll use OLS on the modified model to recover the true effects: $\hat\beta_{GLS} = (X^{*'}X^*)^{-1}X^{*'}Y^*$. What can we say about this new estimator in terms of $X$ and $Y$ (we already know that it is consistent in regards to $X^*$ and $Y^*$)? We know that $$\hat\beta = (X^{*'}X^*)^{-1}X^{*'}Y^* = (X'P'PX)^{-1}X'P'PY = (X'\Omega^{-1}X)^{-1}X'\Omega^{-1} Y $$ Then, \begin{align*} 
\E{\hat\beta} & = \E{(X'\Omega^{-1} X)^{-1}X'\Omega^{-1} Y} \\ 
& = (X'\Omega^{-1} X)^{-1}X'\Omega^{-1}\E{X\beta + e} \\
& = (X'\Omega^{-1} X)^{-1}X'\Omega^{-1}X\beta + (X'\Omega^{-1} X)^{-1}X'\Omega^{-1}\E{e} \\
& = \beta
\end{align*}
and:\begin{align*}
\V{\hat\beta} = \sigma^2(X^{*'}X^*)^{-1} = \sigma^2(X'P'PX)^{-1} = \sigma^2(X'\Omega^{-1} X)^{-1}
\end{align*} And we only need to estimate $\Omega$ in order to get our GLS estimate consistent.

\begin{bclogo}[couleur=blue!10, arrondi=0.1, logo=,ombre=false]{ Weighted Least Squares} 
\begin{small}
Suppose the true model is $$y_i = a + bx_i + cz_i + e_i $$ where $\V{e_i} = \sigma^2w_i^2$. In this context we can guess that $\V{\frac{e_i}{w_i}} = \sigma^2$ and hence $P_{i\times i} = \frac{1}{w_i}$ (meaning that $P$ is a matrix with diagonal terms equal to $1/w_i$). Then, our new model looks like $$PY = Pa + PXb + PZc + Pe$$ or in a clearer way: $$\frac{y_i}{w_i} = \frac{a}{w_i} + b\frac{x_i}{w_i} + c\frac{z_i}{w_i} + \frac{e_i}{w_i} $$ This is called Weighted Least Squares (where the variable $w$ represents the weights put on each variable).
\end{small}
\end{bclogo}

\subsection{White test}

Now that we know what to do in the case of heteroskedasticity, we might want to know how to test if the data is indeed heteroskedastic or not. In order to do this, there are three steps:\begin{enumerate}
\item Regress the original model by OLS and keep the residuals $\hat e_i$
\item Regress the OLS residuals on all variables and their possible interactions (again, by OLS): $$ \hat e_i = a_0 + a_1x_i + a_2z_i + a_3x_i^2 + a_4z_i^2 + a_5x_iz_i $$
\item If we have homoskedasticity, it must be that $\E{eX} = 0$, thus, testing for heteroskedasticity is equivalent to testing whether jointly $a_0 = a_1 = ... = 0$. In order to do that, construct the statistic $nR^2$ from the previous regression and it should follow a chi-squared distribution of $k + 1 + k!$ degrees of freedom. $$nR^2\dconv \chi_{k+1+k!}^2$$
\end{enumerate} This procedure is known as the White test for heteroskedasticity. 

\subsection{White standard errors}

If we do not have a given specification for heteroskedasticity in our model, we will have to fall back on OLS estimation. This causes issues because, while the OLS estimator is consistent, the variance of $\hat\beta$ depends on $\Omega$ which is not defined. We'll have to estimate it.

Recall that $$\V{\hat\beta} = \frac{\sigma^2}{n}\left(\frac{X'X}{n}\right)^{-1}\frac{X'\Omega X}{n}\left(\frac{X'X}{n}\right)^{-1} $$ which, since $\Omega$ is a diagonal matrix, gives $$
\V{\hat\beta} = \frac{1}{n}\left(\frac{X'X}{n}\right)^{-1}\left[\frac{1}{n}\sum_{i}x_ix_i'\sigma_i^2\right]\left(\frac{X'X}{n}\right)^{-1}
$$ Moreover, we know from the LLN that $$\frac{1}{n}\sum_{i}x_ix_i'e_i^2 \to \frac{1}{n}\sum_{i}x_ix_i'\sigma_i^2 $$ Hence we could use the OLS residuals to estimate this and get a consistent estimator for the variance of $\hat\beta$, namely: $$\widehat{\V{\hat\beta}} = \frac{1}{n}\left(\frac{X'X}{n}\right)^{-1}\left[\frac{1}{n}\sum_{i}x_ix_i'\hat e_i^2\right]\left(\frac{X'X}{n}\right)^{-1} $$ Note that relying on the LLN to get the result implies that while White standard errors give a consistent estimator for large samples, it may still be not consistent for small samples.

\section{Autocorrelation}

Autocorrelation is another type of inconsistency of the error term. This time, instead of variance changing with $i$, we have that error terms are correlated with each other: $\E{e_ie_j'} \neq 0$ for $j\neq i$. Because this issue usually arises in temporal contexts, we'll change indexes from $i$ to $t$ and get the following definition of autocorrelation: $\E{e_te_{t-j}}\neq 0$ for $j > 0$. 

\subsection{Correlogram}

We might be interested first in how this autocorrelation is present in the data. For that purpose we'll use a measure of estimated correlation between two periods $t$ and $t-s$ over the whole sample.

\begin{definition}[Sample autocorrelation at lag $s$]
For a given lag $s$, we define the sample autocorrelation, denoted $\hat r_s$ as follows: $$\hat r_s = \frac{\frac{1}{T - s}\sum_{t = s+1}^{T}\hat e_t\hat e_{t-1}}{\frac{1}{T}\sum_{t = 1}^{T}\hat e_t^2} $$ If $\hat r_s$ is big in absolute value, then there is autocorrelation. If $\hat r_s$ is positive, then the autocorrelation is positive, and vice-versa.
\end{definition}

We can represent sample autocorrelation graphically using a correlogram. For each lag, the correlogram will plot the value of the sample correlation in order to compare each one of them. For example, the following graph shows a 4-lag correlogram where sample autocorrelation seems to be decreasing over time: \begin{center}
\includegraphics[scale=0.25]{images/defcorrelogram}
\end{center}

After analysis of sample autocorrelations, one question remains: how many lags are significant in our data? In other words, for how many $j$ do we have autocorrelation with the current error term? In order to answer that question, we define the Ljung-Box Q-statistic that will be used to test the number of significant lags.

\begin{definition}[Ljung-Box Q-statistic]
The Ljung-Box Q-statistic is defined as follows: $$Q = \sum_{s = 1}^{L} \frac{(T+2)(T+s)}{T} \hat r_s^2 $$ Under the null hypothesis (no autocorrelation in the first $L$ lags), we have $Q\dconv \chi_L^2$. Hence it is possible to reject the null if $Q$ does not follow this distribution. Note that in order to carry the test, you should have decided on a $L$ to test in the first place. This could be done with the correlogram for example.
\end{definition}

\subsection{First-order autocorrelation}

In this part of the section on autocorrelation, we'll study the case of a first-order autocorrelation. This model implies that only the first lag ($s=1$) has positive correlation with the instant error. As such, we model our regression as $$Y_t = X_t\beta + e_t $$ $$e_t = \rho e_{t-1} + v_t $$ We assume that $v_t$ is a Gauss-Markov type of error term such that $\E{v_t} = 0$, $\E{v_tv_{t-s}}=0$ for all $s\neq 0$, $\E{v_t^2} = \sigma_v^2$ and hence $\E{vv'} = \sigma^2I_n$. Moreover we assume that the errors are not explosive, meaning that $\vert\rho\vert < 1$. From those assumptions, we can write:\begin{align*}
e_t = \rho e_{t-1} + v_t = \rho ( \rho e_{t-2} + v_{t-1}) + v_t & = \rho^2( \rho e_{t-3} + v_{t-2}) + \rho v_{t-1} + v_t \\ & = \dots \\ & = \sum_{s=0}^{\infty} \rho^s v_{t-s}
\end{align*} and therefore, $$ \E{e_t} = \sum_{s=0}^{\infty} \rho^s \E{v_{t-s}} = 0 $$ 
\begin{align*}
\V{e_t} = \E{\left(\sum_{s=0}^{\infty} \rho^s v_{t-s}\right)^2} = \sum_{s=0}^{\infty} \rho^{2s} \E{v_{t-s}^2}  & = \sigma_{v}^2 \sum_{s=0}^{\infty} (\rho^{2})^s \\ & =  \frac{\sigma_{v}^2}{1 - \rho^2}
\end{align*}

The two last equations imply that the error term $e_t$ is in fact homoskedastic. Because of that fact we can rewrite: $$\E{e_te_{t-s}} = \E{\left(\rho^s e_{t-s} + \sum_{s=0}^{\infty} \rho^s v_{t-s}\right)e_{t-s}} = \rho^s \sigma_e^2 $$ In matrix form, \begin{align*}
\E{ee'} = \sigma_e^2 \begin{bmatrix}
1 & \rho & \hdots & \rho^{T-1} \\
\rho & 1 & \hdots & \rho^{T-2} \\
\vdots & \vdots	& \ddots & \vdots\\
\rho^{T-1} & \rho^{T-2} & \hdots & 1
\end{bmatrix}
\end{align*}

\subsection{GLS and feasible GLS}

As we have seen in the case of heteroskedasticity, knowing the value of $\E{ee'}$ will help us design a matrix $P$ such that $\V{Pe} = \sigma_e^2 I_T$. Here, the coefficient $\rho$ is the key to having a modelthat satisfies GM assumptions.

Suppose $Y_t = a + bX_t + e_t$ is the true model with autocorrelation as presented in the beginning of this section. Then, take the first lag and multiply by $\rho$: $\rho Y_{t-1} = \rho a + \rho b X_{t-1} + \rho e_{t-1}$. By taking the difference: $$Y_t - \rho Y_{t-1} = a - \rho a + bX_t - \rho b X_{t-1} + e_t - \rho e_{t-1} $$ $$\Leftrightarrow Y_t^* = a^* + b X_t^* + v_t $$ which satisfies the Gauss-Markov assumptions. The issue here is that in practice, we do not know the value of $\rho$. Hence we must turn to estimations of this value using a technique called feasible GLS.

The feasible GLS revolves around four steps:\begin{enumerate}
\item Estimate $\hat e_t$ by performing OLS on the original model.
\item Estimate $\hat\rho$ by doing OLS on the error regression.
\item Estimate $\hat\beta$ and $\hat e_t$ by GLS.
\item Repeat steps 2 to 4 until the estimated value $\rho$ has converged.
\end{enumerate}

\subsection{Other lag models}

There are other specifications for the error lags. In particular, three types of models are often used:

\subsubsection{AR$(p)$ processes}

These models function in the same way as the first-lag model described earlier, only this time we allow for $p\geq 1$ lags in the model: $$e_t = \rho_1 e_{t-1} + ... + \rho_p e_{t-p} + v_t $$

\subsubsection{MA$(q)$ processes}

Here, the errors are considered as moving averages of iid shocks that occurred in the last $q$ periods. $$e_t = v_t + \theta_1 v_{t-1} + ... + \theta_q v_{t-1} $$

\subsubsection{ARMA$(p,q)$ processes}

These processes are combinations of AR$(p)$ and MA$(q)$ processes.

\subsection{Newey-West standard errors}

Newey-West standard errors are the autocorrelation analog of White standard errors in the heteroskedastic case. In that sense, they estimate the term $\frac{X'\Omega X}{T}$.

Again, recall that: $$\V{\hat \beta} = \frac{\sigma^2}{T}Q_T^{-1} \frac{X'\Omega X}{T} Q_T^{-1} $$ Since $\Omega$ is not a diagonal matrix anymore, we have that $$\frac{X'\Omega X}{T} = \frac{1}{T}\sum_{t=1}^{T}\sum_{s=1}^{T}\left( \cov{e_t,e_s}\cdot(x_tx_s' + x_sx_t') \right) $$ $$\widehat{\frac{X'\Omega X}{T}} = \frac{1}{T}\sum_{t=1}^{T}\sum_{s=1}^{T}\left(\hat e_t\hat e_s\cdot(x_tx_s' + x_sx_t') \right) $$ and finally, because after $L$ lags, $e_te_{t-L} = 0$, we have: $$\widehat{\frac{X'\Omega X}{T}} = \frac{1}{T}\sum_{t=1}^{T}\sum_{s=T-L+1}^{T}\left(\hat e_t\hat e_s\cdot(x_tx_s' + x_sx_t') \right) $$