\section{Introduction, model and assumptions}

\begin{definition}[Data sample]
A data sample is a set of $n$ observations, denoted by the subscript $i$. Its elements are random variables $(Y_i, X_i)$ where $X_i$ can be a vector.
\end{definition}

In econometrics, we commonly view these observations as iid draws from a common distribution called the data-generating process (or DGP). 

Consider the a model of a variable $Y$ explained by $k$ regressors with $n$ observations: $$Y_i = \beta_1 + \beta_{2}X_{2i} + ... + \beta_kX_{ki} + e_i$$ which can be written in its matrix form:\[
\begin{bmatrix}
    Y_{1}\\
    Y_{2}\\
    \vdots\\
    Y_{n}
\end{bmatrix}
=
\begin{bmatrix}
    1       & X_{21} & X_{31} & \dots & X_{k1} \\
    1       & X_{22} & X_{32} & \dots & X_{k2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1       & X_{2n} & X_{3n} & \dots & X_{kn}
\end{bmatrix}
\cdot
\begin{bmatrix}
    \beta_1 \\
    \vdots \\
    \beta_k
\end{bmatrix}
+ 
\begin{bmatrix}
    e_{1}\\
    e_{2}\\
    \vdots\\
    e_{n}
\end{bmatrix}
\]
\begin{definition}[OLS estimator]
The OLS estimator $\hat\beta$ of the true parameter $\beta$ is the vector that minimizes the sum of squared residuals:\begin{align*}
\hat\beta & \in \operatorname{arg}\min_{\beta} e'e\\
& \in \operatorname{arg}\min_{\beta} (Y -  X\beta)'(Y -  X\beta)
\end{align*} 
\end{definition}

\begin{proposition}
If the matrix given by $X'X$ is invertible, then the value of the OLS estimator $\hat\beta$ is:$$\hat\beta=(X'X)^{-1}X'Y$$
\end{proposition}
\begin{proof}
In order to solve the model, we can simplify the objective function to $Y'Y - Y'\beta X - \beta'X'Y + \beta'X'X\beta $ where each term is a $1\times 1$ matrix. The FOC gives:\begin{align*}
\frac{\partial }{\partial \beta} = 0 & \Leftrightarrow -2X'Y + 2X'X\hat\beta = 0 \\ & \Leftrightarrow X'Y=X'X\hat\beta \\ & \Leftrightarrow \hat\beta=(X'X)^{-1}X'Y
\end{align*}
\end{proof}

\begin{proposition}
In the particular case of $k=2$ so that $y = a + bx +e$. We have that: \begin{align*}
\hat b & = \frac{\sum_{i=1}^{n}(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^{n}(x_i - \bar x)^2}  = \frac{\cov{x,y}}{\V{x}}\\
\hat a & = \bar y - \hat b \bar x
\end{align*}
\end{proposition}

\begin{proof}

\end{proof}

\begin{definition}[Fitted values and residuals]
The fitted value $\hat{Y}$ is defined by:$$\hat{Y} = X\hat{\beta}$$ This variable is not a predictor of $Y$ since it is a function of the sample only. The residuals are $\hat{e} = Y - \hat{Y} = Y - X\hat{\beta}$. They are different from errors $e$ which are unobservable parameters of the regression.
\end{definition}

\begin{proposition}
In a linear regression context, we have: $$X'\hat e = 0$$ This implies that if $X$ contains a column of ones (i.e. there is a constant in the model), then $\avg{1}{n}\hat e_i = 0$.
\end{proposition}

\begin{proof}
We can easily see that:\begin{align*}
X'\hat e & = X'(Y - X\hat\beta) = X'Y - X'X\hat\beta = X'Y - X'X(X'X)^{-1}X'Y = 0 \\ & \Leftrightarrow
\begin{bmatrix}
    1 & 1 & \dots & 1 \\
    X_{21} & X_{22} & \dots & X_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
   X_{k1} & X_{k2} & \dots & X_{kn}
\end{bmatrix}
\cdot 
\begin{bmatrix}
    \hat e_{1}\\
    \hat e_{2}\\
    \vdots\\
    \hat e_{n}
\end{bmatrix} = 0 \Leftrightarrow \begin{bmatrix}
    \sum_{i=1}^{n}\hat e_{i}\\
    \vdots\\
\end{bmatrix} = 0
\end{align*}
\end{proof}

\begin{definition}[Linear estimator]
An estimator is linear in random variables if it can be written as linear transformation of a random vector. In words, it must be equal to a constant matrix multiplied by a random vector: $$\tilde{\beta} = \tilde{C}Y$$
\end{definition}

\begin{definition}[Projection matrix]
The matrix $P = X\tilde{C}$ is a projection matrix. Its properties are that:\begin{itemize}
\item $PX = X$
\item $P=P'$
\item $PP = P$
\item $\operatorname{tr}(P) = k$
\end{itemize} 
\end{definition}

\begin{proposition}
The projection matrix $P$ can be used to recover the fitted values with $$PY = \hat Y$$
\end{proposition}
\begin{proof}
We have $PY = X(X'X)^{-1}X'Y = X\hat\beta = \hat Y$
\end{proof}

\begin{definition}[Orthogonal projection matrix]
Let $M = I_n - P$, $M$ is called the orthogonal projection matrix since $MX = 0$. Other properties of $M$ include:\begin{itemize}
\item $MP = 0$
\item $\operatorname{tr}(M) = n-k$
\item $MY = Y - PY = Y - \hat Y = \hat e $
\item $\hat e = MY = M(X\beta + e) = Me$
\end{itemize}
\end{definition}

\begin{definition}[Estimator of the error variance]
The moment estimator of the variance $\sigma^2$ would be: $$\tilde{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n e_i^2 $$ However, $e_i$ is never observed and cannot be used. Let's substitute for $\hat{e_i}$ after OLS estimation. We get the feasible variance estimator:$$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n \hat e_i^2 $$ Alternatively, we can write $\tilde{\sigma}^2 = n^{-1}e'e$ and $\hat{\sigma}^2 = n^{-1}\hat{e}'\hat{e} = n^{-1}Y'MMY = n^{-1}e'MMe = n^{-1}e'Me$. A nice property of this is that:\begin{align*}
\tilde{\sigma}^2 - \hat{\sigma}^2 = n^{-1}e'e - n^{-1}e'Me & = n^{-1}e'(I_n - M)e \\
& = n^{-1}e'Pe \\
& \geq 0
\end{align*}which means that $\tilde{\sigma}^2 \geq \hat{\sigma}^2$.
\end{definition}

\begin{definition}[$R^2$ and analysis-of-variance]
We can measure the variance of the model with a variable called $R^2$. Write $$Y = PY + MY = \hat{Y} + \hat{e}$$ It follows that $$Y'Y = \hat{Y}'\hat{Y} + 2\hat{Y}'\hat{e} + \hat{e}'\hat{e} = \hat{Y}'\hat{Y} + \hat{e}'\hat{e} $$
And hence $Y - \bar{Y} = \hat{Y} - \bar{Y} + \hat{e} \Rightarrow (Y - \bar{Y})'(Y - \bar{Y}) = (\hat{Y} - \bar{Y})'(\hat{Y} - \bar{Y}) + 2(\hat{Y} - \bar{Y})'\hat{e} + \hat{e}'\hat{e}$ which gives $$\V{Y} = \V{\hat{Y}} + \V{\hat{e}}$$ Finally, we define as $R^2$ the proportion of variation of $Y$ explained by a variation in $\hat{Y}$: $$R^2 = \frac{\V{\hat{Y}}}{\V{Y}} = 1 - \frac{\V{\hat{e}}}{\V{Y}} $$
\end{definition}

We have already seen that, in order to get a solution for our OLS estimator we need the assumption of non-singularity of $X'X$. In the same spirit, we will need other assumptions in order to draw out the properties of $\hat\beta$ whether in finite or infinite samples. The assumptions that are going to be described here represent the minimal assumptions that one can make ; we'll see what they imply and how to relax them in the following sections.

\begin{definition}[Classical assumptions]
The following assumptions on our model are called the classical assumptions:
\begin{itemize}
\item[\textbf{A1}]\textbf{Linearity and correct specification:} the model must be correctly specified as linear in parameters (no $\beta^2$). In matrix form, the model must be represented by $$Y = X\beta + e$$
\item[\textbf{A2}]\textbf{No randomness in $X$:} the data in $X$ is not random, meaning that it would be the exact same if we took another sample of the population. Note that this assumption is quite strong but is not necessary. Indeed, if $X$ is random, than assumption A4 will need to be conditional on $X$.
\item[\textbf{A3}]\textbf{Non-singularity of $X'X$:} since the OLS estimator takes the inverse of $X'X$. For it to be non-singular, it must be that:\begin{itemize}
\item $n>k$: there are more observations than explanatory variables (no over-idenfication), and
\item $\operatorname{rank}(X) = k$ (no multicollinearity in $X$)
\end{itemize}
\item[\textbf{A4}]\textbf{The errors are mean-zero and homoskedastic:} in particular, $$ \E{e_i} = \E{e} = 0 \text{ and } \V{e} = \Omega = \sigma^2I_n $$
This property also means that there is no autocorrelation in the data $\cov{e_i, e_j} = 0$ for all $i\neq j$.

If the data is random, then we need that: $$ \E{e_i|X_i} = \E{e|X} = 0 \text{ and } \V{e|X} = \Omega = \sigma^2I_n $$
\end{itemize}
\end{definition}


\section{Finite sample properties of OLS estimation}
Thanks to these four assumptions, we will be able to discuss more in depth the properties of our OLS estimator, first in finite samples.

\begin{proposition}[Unbiasedness of OLS estimator]
Under assumptions A1-A4, the OLS estimator $\hat\beta$ is unbiased.
\end{proposition}
\begin{proof}
We already know that $\hat\beta = (X'X)^{-1}X'Y$. Therefore, \begin{align*}
\E{\hat\beta} = \E{(X'X)^{-1}X'Y} & = \E{(X'X)^{-1}X'(X\beta + e)}\\
& = \E{(X'X)^{-1}X'X\beta + (X'X)^{-1}X'e}\\
& = \E{\beta + (X'X)^{-1}X'e}\\
& = \beta + (X'X)^{-1}X'\E{e}\\
& = \beta
\end{align*}
\end{proof}

\begin{proposition}[Unbiasedness of linear estimators]
A linear estimator is unbiased if and only if its associated transformation matrix $\tilde{C}$ is such that $$\tilde{C}X = I_k$$
\end{proposition}
\begin{proof}
In the case of linear regression, we'd have \begin{align*}
\tilde{\beta} = \tilde{C}(X\beta + e) = \tilde{C}X\beta + \tilde{C}e \Rightarrow \E{\tilde{\beta}} & = \tilde{C}X\beta + \tilde{C}\E{e} \\
& = \tilde{C}X\beta = \beta \text{ if } \tilde{C}X = I_k
\end{align*}This is verified in the OLS since $\tilde{C}X = (X'X)^{-1}X'X = I_k$
\end{proof}

\begin{proposition}[Variance of linear estimators]
The variance of a homoskedastic, non-autocorrelated linear estimator is given by $\V{\tilde{\beta}} =  \sigma^2(\tilde{C}\tilde{C}')$. In the particular case of the OLS estimator, we have $\V{\hat{\beta}} = \sigma^2(X'X)^{-1}$
\end{proposition}
\begin{proof}
The proof is trivial and follows the properties of the variance.
\begin{align*}
\V{\tilde{\beta}} = \V{\tilde{C}X\beta + \tilde{C}e} = \V{\tilde{C}e} & = \tilde{C}\V{e}\tilde{C}'\\
& = \tilde{C}\Omega\tilde{C}'\\
& = \tilde{C}\sigma^2 I_n\tilde{C}'\\
& = \sigma^2(\tilde{C}\tilde{C}')
\end{align*}
In the case of $\hat\beta$, we have $\sigma^2(\tilde{C}\tilde{C}') = \sigma^2((X'X)^{-1}X'((X'X)^{-1}X')') = \sigma^2(X'X)^{-1}$
\end{proof}

\begin{proposition}[OLS as the Best Linear Unbiased Estimator]
Under assumptions A1-A4, the OLS estimator $\hat\beta$ is the Best Linear Unbiased Estimator (BLUE). This property means that, among the class of linear unbiased estimators, the OLS estimator is the most efficient one.
\end{proposition}
\begin{proof}
Consider the alternative linear estimator $CY = [(X'X)^{-1}X' + D]Y$, where $D$ is a constant. We need that $CY$ is unbiased, meaning that: \begin{align*}
\E{CY} = \E{[(X'X)^{-1}X' + D]Y} & = \E{(X'X)^{-1}X'Y} + \E{DY} \\
& = \beta + \E{DX\beta} + \E{De} \\
& = (I_k + DX)\beta \Rightarrow DX = 0
\end{align*}
Now, the variance of this estimator is given by: \begin{align*}
\V{CY} & = C\V{Y}C'\\ & = \sigma^2 CC' \\ & = \sigma^2 [(X'X)^{-1}X' + D][(X'X)^{-1}X' + D]' \\
& = \sigma^2 [(X'X)^{-1}X'X(X'X)^{-1} + (X'X)^{-1}(DX)' \\ & + (DX)(X'X)^{-1} + DD'] \\
& = \sigma^2 [(X'X)^{-1} + DD'] \geq \sigma^2(X'X)^{-1}
\end{align*}
\end{proof}

\begin{bclogo}[couleur=blue!10, arrondi=0.1, logo=,ombre=false]{ Practical considerations on the data} 
\begin{small}
Assume the model is $$y_i = a + bx_i + e_i$$
Then, we can show that \begin{itemize}
\item $\V{\hat b} = $
\item $\V{\hat a} = $
\item $\cov{\hat b, \hat a} = $
\end{itemize}
Analyzing the data we can find some interesting properties for our model.

For example, if $\sigma^2$ is small, all three variances and covariance will be small as well. A lower $\sigma$ implies a more efficient model.

Now, if $n$ is big, the effect is the same, since all variances will be smaller, our model will be more accurate.

Again, the implications are the same with greater values of $x_i - \bar x$.

Finally, we can see that the covariance between the two estimators indicate how their errors are related. If the covariance is high and positive, then a mistake in the estimation of $\hat b$ will lead to the same mistakes in $\hat a$.
\end{small}
\end{bclogo}

Now we have seen that $\V{\hat{\beta}}$ is given by $\sigma^2(X'X)^{-1}$. However, in actual situations, the true value $\sigma$ is unknown and we would need to estimate it.

\begin{proposition}[Properties of the variance estimator]
Let $\hat\sigma^2$ be the sample moment estimator. This estimator is biased and: $$\E{\hat{\sigma}^2} = \sigma^2\left(\frac{n-k}{n}\right)$$
\end{proposition}
\begin{proof}
\begin{align*}
\E{\hat\sigma^2} = \E{n^{-1}e'Me} = n^{-1}\E{\operatorname{tr}(Mee')} = n^{-1}\operatorname{tr}(M\E{ee'}) & = n^{-1}\operatorname{tr}(M\Omega) \\ & = n^{-1}\sigma^2(n-k)
\end{align*}
\end{proof}

\begin{definition}[Adjusted sample variance]
We define $s^2$ to be the adjusted sample estimator of the variance, in short the adjusted sample variance, such that: $$s^2 = \frac{\hat e'\hat e}{n-k}=\frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{n-k}$$ This implies that this time we have: $\E{s^2} = \sigma^2$. 
Hence, we can use this estimator to estimate the variance of our OLS estimator $\hat\beta$: $$\widehat{\V{\hat{\beta}}} = s^2(X'X)^{-1}$$ Each parameter $\hat{\beta}_k$'s variance would be the $(k,k)$th element of the matrix.
\end{definition}

Again, we find ourselves with more information about $\hat{\beta}$, namely its mean and variance, but not enough information to get the whole distribution of $\hat{\beta}$. We know that $\hat{\beta} = \beta + (X'X)^{-1}X'e$ where the distribution $e$ is the only unknown. We will need a new assumption.
\begin{definition}[Normality of the error term]
Assuming all classical assumptions hold. We add the assumption (\textbf{A5}) that the error term $e_i$ follows a normal distribution of mean $0$ and variance $\sigma^2I_n$.

Following this assumption, we now know that $Y\sim N(X'\beta, \sigma^2 I_n)$ and $\hat{\beta}\sim N(\beta,\sigma^2(X'X)^{-1})$.
\end{definition}

\begin{proposition}
Let $V_j$ denote the $(j,j)$th element of the matrix $(X'X)^{-1}$. Then, $\hat{\beta}_j\sim N(\beta_j, \sigma^2V_j)$ where $\sigma^2$ can be estimated with $s^2$.

Therefore, $$\frac{\hat{\beta}_j -\beta_j}{\sqrt{\sigma^2V_j}}\sim N(0,1)$$ while, $$\frac{\hat{\beta}_j -\beta_j}{\sqrt{s^2V_j}}\sim t_{n-k}$$
\end{proposition}

This last fact can be used in interval estimation as it implies that: $$\operatorname{Pr}(\hat{\beta}_j - t_{\alpha /2}\frac{S}{\sqrt{V_j}} $$

\begin{proposition}[Moments of the residuals]
Let the residuals of the regression be $\hat{e} = Me$ as we've seen before. We have that:\begin{itemize}
\item $\E{\hat{e}} = 0$
\item $\V{\hat{e}} = M\sigma^2$
\end{itemize}
\end{proposition}
\begin{proof}
We have that: $\E{\hat{e}} = \E{Me} = M\E{e} = 0$. And $\V{\hat{e}} = \V{Me} = M\V{e}M' = M\Omega M = M\sigma^2I_nM = \sigma^2MM = \sigma^2$
\end{proof}

\

\section{Asymptotic properties of OLS estimation}

\begin{proposition}[Consistency of $\hat\beta$]
Let $Q_n = \frac{X'X}{n}$, which is a non-singular, positive definite matrix (from A3). Moreover, let its limit $Q = \lim_{n\to\infty} Q_n$ exist.

Consider that $\E{\hat{\beta}}=\beta$ and $\V{\hat{\beta}} = \frac{\sigma^2}{n}Q_n^{-1}$. Then, \begin{itemize}
\item $\lim_{n\to\infty}\E{\hat{\beta}} = \beta$
\item $\lim_{n\to\infty}\V{\hat{\beta}} = \lim_{n\to\infty} \frac{\sigma^2}{n}Q_n^{-1} = 0$
\end{itemize}
and therefore, $\hat{\beta}\msconv\beta$.
\end{proposition}

\begin{proposition}[Root-n-consistency of $\hat\beta$]

\end{proposition}